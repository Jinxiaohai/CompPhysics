%%
%% Automatically generated file from DocOnce source
%% (https://github.com/hplgit/doconce/)
%%
%%


%-------------------- begin preamble ----------------------

\documentclass[%
twoside,                 % oneside: electronic viewing, twoside: printing
final,                   % or draft (marks overfull hboxes, figures with paths)
10pt]{article}

\listfiles               % print all files needed to compile this document

\usepackage{relsize,makeidx,color,setspace,amsmath,amsfonts}
\usepackage[table]{xcolor}
\usepackage{bm,microtype}

\usepackage{fancyvrb} % packages needed for verbatim environments
\usepackage{minted}
\usemintedstyle{default}

\usepackage[T1]{fontenc}
%\usepackage[latin1]{inputenc}
\usepackage{ucs}
\usepackage[utf8x]{inputenc}

\usepackage{lmodern}         % Latin Modern fonts derived from Computer Modern

% Hyperlinks in PDF:
\definecolor{linkcolor}{rgb}{0,0,0.4}
\usepackage{hyperref}
\hypersetup{
    breaklinks=true,
    colorlinks=true,
    linkcolor=linkcolor,
    urlcolor=linkcolor,
    citecolor=black,
    filecolor=black,
    %filecolor=blue,
    pdfmenubar=true,
    pdftoolbar=true,
    bookmarksdepth=3   % Uncomment (and tweak) for PDF bookmarks with more levels than the TOC
    }
%\hyperbaseurl{}   % hyperlinks are relative to this root

\setcounter{tocdepth}{2}  % number chapter, section, subsection

\usepackage[framemethod=TikZ]{mdframed}

% --- begin definitions of admonition environments ---

% --- end of definitions of admonition environments ---

% prevent orhpans and widows
\clubpenalty = 10000
\widowpenalty = 10000

\newenvironment{doconceexercise}{}{}
\newcounter{doconceexercisecounter}


% ------ header in subexercises ------
%\newcommand{\subex}[1]{\paragraph{#1}}
%\newcommand{\subex}[1]{\par\vspace{1.7mm}\noindent{\bf #1}\ \ }
\makeatletter
% 1.5ex is the spacing above the header, 0.5em the spacing after subex title
\newcommand\subex{\@startsection{paragraph}{4}{\z@}%
                  {1.5ex\@plus1ex \@minus.2ex}%
                  {-0.5em}%
                  {\normalfont\normalsize\bfseries}}
\makeatother


% --- end of standard preamble for documents ---


% insert custom LaTeX commands...

\raggedbottom
\makeindex

%-------------------- end preamble ----------------------

\begin{document}

% endif for #ifdef PREAMBLE


% ------------------- main content ----------------------



% ----------------- title -------------------------

\thispagestyle{empty}

\begin{center}
{\LARGE\bf
\begin{spacing}{1.25}
Conjugate gradient methods and other optimization methods
\end{spacing}
}
\end{center}

% ----------------- author(s) -------------------------

\begin{center}
{\bf Morten Hjorth-Jensen, National Superconducting Cyclotron Laboratory and Department of Physics and Astronomy, Michigan State University, East Lansing, MI 48824, USA {\&} Department of Physics, University of Oslo, Oslo, Norway${}^{}$} \\ [0mm]
\end{center}

    \begin{center}
% List of all institutions:
\end{center}
    
% ----------------- end author(s) -------------------------

\begin{center} % date
Spring 2015
\end{center}

\vspace{1cm}


% !split
\subsection{Motivation}

% --- begin paragraph admon ---
\paragraph{}
Our aim with this part of the project is to be able to
\begin{itemize}
\item find an optimal value for the variational parameters using only some few Monte Carlo cycles

\item use these optimal values for the variational parameters to perform a large-scale Monte Carlo calculation
\end{itemize}

\noindent
To achieve this will look at methods like \emph{Steepest descent} and the \emph{conjugate gradient method}. Both these methods allow us to find
the minima of a multivariable  function like our energy (function of several variational parameters). 
Alternatively, you can always use Newton's method. In particular, since we will only have one variational parameter,
Newton's method can be easily used in finding the minimum of the local energy.
% --- end paragraph admon ---



% !split
\subsection{Simple example and demonstration}

% --- begin paragraph admon ---
\paragraph{}
Let us illustrate what is needed in our calculations using a simple example, the harmonic oscillator in one dimension.
For the harmonic oscillator in one-dimension we have a  trial wave function and probability
\begin{equation*}
\psi_T(x) = e^{-\alpha^2 x^2} \qquad P_T(x)dx = \frac{e^{-2\alpha^2 x^2}dx}{\int dx e^{-2\alpha^2 x^2}}
\end{equation*}
with $\alpha$ being the variational parameter. 
We obtain then the following local energy
\begin{equation*}
E_L[\alpha] = \alpha^2+x^2\left(\frac{1}{2}-2\alpha^2\right),
\end{equation*}
which results in the expectation value for the local energy
\begin{equation*}
\langle  E_L[\alpha]\rangle = \frac{1}{2}\alpha^2+\frac{1}{8\alpha^2}
\end{equation*}
% --- end paragraph admon ---





% !split
\subsection{Simple example and demonstration}

% --- begin paragraph admon ---
\paragraph{}
The derivative of the energy with respect to $\alpha$ gives
\begin{equation*}
\frac{d\langle  E_L[\alpha]\rangle}{d\alpha} = \alpha-\frac{1}{4\alpha^3}
\end{equation*}
and a second derivative which is always positive (meaning that we find a minimum)
\begin{equation*}
\frac{d^2\langle  E_L[\alpha]\rangle}{d\alpha^2} = 1+\frac{3}{4\alpha^4}
\end{equation*}
The condition
\begin{equation*}
\frac{d\langle  E_L[\alpha]\rangle}{d\alpha} = 0,
\end{equation*}
gives the optimal $\alpha=1/\sqrt{2}$, as expected.
% --- end paragraph admon ---



% !split


% --- begin exercise ---
\begin{doconceexercise}
\refstepcounter{doconceexercisecounter}

\subsection*{Exercise \thedoconceexercisecounter: Find the local energy for the harmonic oscillator}



\subex{a)}
Derive the local energy for the harmonic oscillator in one dimension and find its expectation value.

\subex{b)}
Show also that the optimal value of optimal $\alpha=1/\sqrt{2}$.


\end{doconceexercise}
% --- end exercise ---


% !split
\subsection{Variance in the simple model}

% --- begin paragraph admon ---
\paragraph{}
We can also minimize the variance. In our simple model the variance is
\begin{equation*}
\sigma^2[\alpha] = \frac{1}{2}\alpha^4-\frac{1}{4}+\frac{1}{32\alpha^4},
\end{equation*}
with first derivative
\begin{equation*}
\frac{d \sigma^2[\alpha]}{d\alpha} = 2\alpha^3-\frac{1}{8\alpha^5}
\end{equation*}
and a second derivative which is always positive
\begin{equation*}
\frac{d^2\sigma^2[\alpha]}{d\alpha^2} = 6\alpha^2+\frac{5}{8\alpha^6}
\end{equation*}
% --- end paragraph admon ---




% !split
\subsection{Computing the derivatives}

% --- begin paragraph admon ---
\paragraph{}
In general we end up computing the expectation value of the energy in terms 
of some parameters $\alpha_0,\alpha_1,\dots,\alpha_n$
and we search for a minimum in this multi-variable parameter space.  
This leads to an energy minimization problem \emph{where we need the derivative of the energy as a function of the variational parameters}.

In the above example this was easy and we were able to find the expression for the derivative by simple derivations. 
However, in our actual calculations the energy is represented by a multi-dimensional integral with several variational parameters.
How can we can then obtain the derivatives of the energy with respect to the variational parameters without having 
to resort to expensive numerical derivations?
% --- end paragraph admon ---




% !split
\subsection{Expressions for finding the derivatives of the local energy}

% --- begin paragraph admon ---
\paragraph{}

To find the derivatives of the local energy expectation value as function of the variational parameters, we can use the chain rule and the hermiticity of the Hamiltonian.  

Let us define 
\[
\bar{E}_{\alpha}=\frac{d\langle  E_L[\alpha]\rangle}{d\alpha}.
\]
as the derivative of the energy with respect to the variational parameter $\alpha$ (we limit ourselves to one parameter only).
In the above example this was easy and we obtain a simple expression for the derivative.
We define also the derivative of the trial function (skipping the subindex $T$) as 
\[
\bar{\psi}_{\alpha}=\frac{d\psi[\alpha]\rangle}{d\alpha}.
\]
% --- end paragraph admon ---





% !split
\subsection{Derivatives of the local energy}

% --- begin paragraph admon ---
\paragraph{}
The elements of the gradient of the local energy are then (using the chain rule and the hermiticity of the Hamiltonian)
\[
\bar{E}_{\alpha} = 2\left( \langle \frac{\bar{\psi}_{\alpha}}{\psi[\alpha]}E_L[\alpha]\rangle -\langle \frac{\bar{\psi}_{\alpha}}{\psi[\alpha]}\rangle\langle E_L[\alpha] \rangle\right).
\]
From a computational point of view it means that you need to compute the expectation values of 
\[
\langle \frac{\bar{\psi}_{\alpha}}{\psi[\alpha]}E_L[\alpha]\rangle,
\]
and
\[
\langle \frac{\bar{\psi}_{\alpha}}{\psi[\alpha]}\rangle\langle E_L[\alpha]\rangle
\]
% --- end paragraph admon ---




% !split


% --- begin exercise ---
\begin{doconceexercise}
\refstepcounter{doconceexercisecounter}

\subsection*{Exercise \thedoconceexercisecounter: General expression for the derivative of the energy}



\subex{a)}
Show that 
\[
\bar{E}_{\alpha} = 2\left( \langle \frac{\bar{\psi}_{\alpha}}{\psi[\alpha]}E_L[\alpha]\rangle -\langle \frac{\bar{\psi}_{\alpha}}{\psi[\alpha]}\rangle\langle E_L[\alpha] \rangle\right).
\]

\subex{b)}
Find the corresponding expression for the variance.


\end{doconceexercise}
% --- end exercise ---


% !split


% --- begin exercise ---
\begin{doconceexercise}
\refstepcounter{doconceexercisecounter}

\subsection*{Exercise \thedoconceexercisecounter: Helium atom and hydrogen-like functions}



\subex{a)}
The expectation value for the local energy of the Helium atom with a simple Slater determinant is given by (project 1)
\[
\langle E_{L} \rangle = \alpha^2-2\alpha\left(Z-\frac{5}{16}\right)
\]
Show that its derivative is 
\begin{equation*}
\frac{d\langle E_{L}[\alpha] \rangle}{d\alpha} = 2\alpha-2\left(Z-\frac{5}{16}\right).
\end{equation*}

\subex{b)}
Use this result when you test your numerical derivative, using either the steepest descent method, Newton's method or the conjugate
gradient method.




\end{doconceexercise}
% --- end exercise ---


% !split
\subsection{Conjugate gradient (CG) method}

% --- begin paragraph admon ---
\paragraph{}
The success of the CG method  for finding solutions of non-linear problems is based
on the theory of conjugate gradients for linear systems of equations. It belongs
to the class of iterative methods for solving problems from linear algebra of the type
\begin{equation*}
  \hat{A}\hat{x} = \hat{b}.
\end{equation*}
In the iterative process we end up with a problem like

\begin{equation*}
  \hat{r}= \hat{b}-\hat{A}\hat{x},
\end{equation*}
where $\hat{r}$ is the so-called residual or error in the iterative process.

When we have found the exact solution, $\hat{r}=0$.
% --- end paragraph admon ---




% !split
\subsection{Conjugate gradient method}

% --- begin paragraph admon ---
\paragraph{}

The residual is zero when we reach the minimum of the quadratic equation
\begin{equation*}
  P(\hat{x})=\frac{1}{2}\hat{x}^T\hat{A}\hat{x} - \hat{x}^T\hat{b},
\end{equation*}
with the constraint that the matrix $\hat{A}$ is positive definite and symmetric.
If we search for a minimum of the quantum mechanical  variance, then the matrix 
$\hat{A}$, which is called the Hessian, is given by the second-derivative of the function we want to minimize.  This quantity is always positive definite.  In our case this corresponds normally to the second derivative of the energy.
% --- end paragraph admon ---







% !split
\subsection{Conjugate gradient method, Newton's method first}

% --- begin paragraph admon ---
\paragraph{}
We seek the minimum of the energy or the variance as function of various variational parameters. 
In our case we have thus a function $f$ whose minimum we are seeking.
In Newton's method we set $\nabla f = 0$ and we can thus compute the next iteration point
\begin{equation*}
\hat{x}-\hat{x}_i=\hat{A}^{-1}\nabla f(\hat{x}_i).
\end{equation*}
Subtracting this equation from that of $\hat{x}_{i+1}$ we have
\begin{equation*}
\hat{x}_{i+1}-\hat{x}_i=\hat{A}^{-1}(\nabla f(\hat{x}_{i+1})-\nabla f(\hat{x}_i)).
\end{equation*}
% --- end paragraph admon ---



% !split
\subsection{Simple example and demonstration}

% --- begin paragraph admon ---
\paragraph{}
The function $f$ can be either the energy or the variance.  If we choose the energy then we have
\begin{equation*}
\hat{\alpha}_{i+1}-\hat{\alpha}_i=\hat{A}^{-1}(\nabla E(\hat{\alpha}_{i+1})-\nabla E(\hat{\alpha}_i)).
\end{equation*}
In the simple harmonic oscillator model, the gradient and the Hessian $\hat{A}$ are
\begin{equation*}
\frac{d\langle  E_L[\alpha]\rangle}{d\alpha} = \alpha-\frac{1}{4\alpha^3}
\end{equation*}
and a second derivative which is always positive (meaning that we find a minimum)
\begin{equation*}
\hat{A}= \frac{d^2\langle  E_L[\alpha]\rangle}{d\alpha^2} = 1+\frac{3}{4\alpha^4}
\end{equation*}
% --- end paragraph admon ---



% !split
\subsection{Simple example and demonstration}

% --- begin paragraph admon ---
\paragraph{}
We get then
\begin{equation*}
\alpha_{i+1}=\frac{4}{3}\alpha_i-\frac{\alpha_i^4}{3\alpha_{i+1}^3},
\end{equation*}
which can be rewritten as
\begin{equation*}
\alpha_{i+1}^4-\frac{4}{3}\alpha_i\alpha_{i+1}^4+\frac{1}{3}\alpha_i^4.
\end{equation*}
% --- end paragraph admon ---



% !split
\subsection{Conjugate gradient method}

% --- begin paragraph admon ---
\paragraph{}
In the CG method we define so-called conjugate directions and two vectors 
$\hat{s}$ and $\hat{t}$
are said to be
conjugate if
\begin{equation*}
\hat{s}^T\hat{A}\hat{t}= 0.
\end{equation*}
The philosophy of the CG method is to perform searches in various conjugate directions
of our vectors $\hat{x}_i$ obeying the above criterion, namely
\begin{equation*}
\hat{x}_i^T\hat{A}\hat{x}_j= 0.
\end{equation*}
Two vectors are conjugate if they are orthogonal with respect to 
this inner product. Being conjugate is a symmetric relation: if $\hat{s}$ is conjugate to $\hat{t}$, then $\hat{t}$ is conjugate to $\hat{s}$.
% --- end paragraph admon ---



% !split
\subsection{Conjugate gradient method}

% --- begin paragraph admon ---
\paragraph{}
An example is given by the eigenvectors of the matrix
\begin{equation*}
\hat{v}_i^T\hat{A}\hat{v}_j= \lambda\hat{v}_i^T\hat{v}_j,
\end{equation*}
which is zero unless $i=j$.
% --- end paragraph admon ---




% !split
\subsection{Conjugate gradient method}

% --- begin paragraph admon ---
\paragraph{}
Assume now that we have a symmetric positive-definite matrix $\hat{A}$ of size
$n\times n$. At each iteration $i+1$ we obtain the conjugate direction of a vector
\begin{equation*}
\hat{x}_{i+1}=\hat{x}_{i}+\alpha_i\hat{p}_{i}. 
\end{equation*}
We assume that $\hat{p}_{i}$ is a sequence of $n$ mutually conjugate directions. 
Then the $\hat{p}_{i}$  form a basis of $R^n$ and we can expand the solution 
$  \hat{A}\hat{x} = \hat{b}$ in this basis, namely

\begin{equation*}
  \hat{x}  = \sum^{n}_{i=1} \alpha_i \hat{p}_i.
\end{equation*}
% --- end paragraph admon ---



% !split
\subsection{Conjugate gradient method}

% --- begin paragraph admon ---
\paragraph{}
The coefficients are given by
\begin{equation*}
    \mathbf{A}\mathbf{x} = \sum^{n}_{i=1} \alpha_i \mathbf{A} \mathbf{p}_i = \mathbf{b}.
\end{equation*}
Multiplying with $\hat{p}_k^T$  from the left gives

\begin{equation*}
  \hat{p}_k^T \hat{A}\hat{x} = \sum^{n}_{i=1} \alpha_i\hat{p}_k^T \hat{A}\hat{p}_i= \hat{p}_k^T \hat{b},
\end{equation*}
and we can define the coefficients $\alpha_k$ as

\begin{equation*}
    \alpha_k = \frac{\hat{p}_k^T \hat{b}}{\hat{p}_k^T \hat{A} \hat{p}_k}
\end{equation*}
% --- end paragraph admon ---



% !split
\subsection{Conjugate gradient method and iterations}

% --- begin paragraph admon ---
\paragraph{}

If we choose the conjugate vectors $\hat{p}_k$ carefully, 
then we may not need all of them to obtain a good approximation to the solution 
$\hat{x}$. 
We want to regard the conjugate gradient method as an iterative method. 
This will us to solve systems where $n$ is so large that the direct 
method would take too much time.

We denote the initial guess for $\hat{x}$ as $\hat{x}_0$. 
We can assume without loss of generality that
\begin{equation*}
\hat{x}_0=0,
\end{equation*}
or consider the system
\begin{equation*}
\hat{A}\hat{z} = \hat{b}-\hat{A}\hat{x}_0,
\end{equation*}
instead.
% --- end paragraph admon ---




% !split
\subsection{Conjugate gradient method}

% --- begin paragraph admon ---
\paragraph{}
One can show that the solution $\hat{x}$ is also the unique minimizer of the quadratic form
\begin{equation*}
  f(\hat{x}) = \frac{1}{2}\hat{x}^T\hat{A}\hat{x} - \hat{x}^T \hat{x} , \quad \hat{x}\in\mathbf{R}^n. 
\end{equation*}
This suggests taking the first basis vector $\hat{p}_1$ 
to be the gradient of $f$ at $\hat{x}=\hat{x}_0$, 
which equals
\begin{equation*}
\hat{A}\hat{x}_0-\hat{b},
\end{equation*}
and 
$\hat{x}_0=0$ it is equal $-\hat{b}$.
The other vectors in the basis will be conjugate to the gradient, 
hence the name conjugate gradient method.
% --- end paragraph admon ---




% !split
\subsection{Conjugate gradient method}

% --- begin paragraph admon ---
\paragraph{}
Let  $\hat{r}_k$ be the residual at the $k$-th step:
\begin{equation*}
\hat{r}_k=\hat{b}-\hat{A}\hat{x}_k.
\end{equation*}
Note that $\hat{r}_k$ is the negative gradient of $f$ at 
$\hat{x}=\hat{x}_k$, 
so the gradient descent method would be to move in the direction $\hat{r}_k$. 
Here, we insist that the directions $\hat{p}_k$ are conjugate to each other, 
so we take the direction closest to the gradient $\hat{r}_k$  
under the conjugacy constraint. 
This gives the following expression
\begin{equation*}
\hat{p}_{k+1}=\hat{r}_k-\frac{\hat{p}_k^T \hat{A}\hat{r}_k}{\hat{p}_k^T\hat{A}\hat{p}_k} \hat{p}_k.
\end{equation*}
% --- end paragraph admon ---



% !split
\subsection{Conjugate gradient method}

% --- begin paragraph admon ---
\paragraph{}
We can also  compute the residual iteratively as
\begin{equation*}
\hat{r}_{k+1}=\hat{b}-\hat{A}\hat{x}_{k+1},
 \end{equation*}
which equals
\begin{equation*}
\hat{b}-\hat{A}(\hat{x}_k+\alpha_k\hat{p}_k),
 \end{equation*}
or
\begin{equation*}
(\hat{b}-\hat{A}\hat{x}_k)-\alpha_k\hat{A}\hat{p}_k,
 \end{equation*}
which gives

\begin{equation*}
\hat{r}_{k+1}=\hat{r}_k-\hat{A}\hat{p}_{k},
 \end{equation*}
% --- end paragraph admon ---




% !split
\subsection{Using the conjugate gradient method}

% --- begin paragraph admon ---
\paragraph{}
\begin{itemize}
  \item Start your program with calling a function which implements for example the  CG method  or the steepest descent method. 

  \item This function needs a function for the expectation value of the local energy and the derivative of the local energy.

  \item Your function \textbf{func} is now the Metropolis part with a call to the local energy function. For every call to the function \textbf{func} many practitionser use 1000-10000 Monte Carlo cycles for the trial wave function.

  \item This gives an expectation value for the energy which is returned by the function \textbf{func}.

  \item When one calls the local energy one also computes the first derivative of the expectaction value of the local energy  
\end{itemize}

\noindent
\begin{equation*} \frac{d\langle E_{L}[\alpha] \rangle}{d\alpha}= 2\langle \frac{\bar{\psi_T}_{\alpha}}{\psi_T[\alpha]}\left(E_L[\alpha]-\langle  E_L[\alpha]\rangle\right)\rangle.
\end{equation*}
The following two codes, to be found in the \href{{https://github.com/CompPhysics/ComputationalPhysics2/tree/gh-pages/doc/pub/cg/programs/c%2B%2B}}{programs folder} as well, implement first the conjugate gradient and steepest descent method to a simple $2\times 2$ problem. Finally, we include a program based on a more advanced version of the conjugate gradient method.
% --- end paragraph admon ---






% !split
\subsection{Simple codes for  steepest descent and conjugate gradient using a $2\times 2$ matrix}

% --- begin paragraph admon ---
\paragraph{}
\begin{minted}[fontsize=\fontsize{9pt}{9pt},linenos=false,mathescape,baselinestretch=1.0,fontfamily=tt,xleftmargin=7mm]{c++}
#include <cmath>
#include <iostream>
#include <fstream>
#include <iomanip>
#include "vectormatrixclass.h"
using namespace  std;
//   Main function begins here
int main(int  argc, char * argv[]){
  int dim = 2;
  Vector x(dim),xsd(dim), b(dim),x0(dim);
  Matrix A(dim,dim);
  
  // Set our initial guess
  x0(0) = x0(1) = 0;
  // Set the matrix  
  A(0,0) =  3;    A(1,0) =  2;   A(0,1) =  2;   A(1,1) =  6; 
  b(0) = 2; b(1) = -8;
  cout << "The Matrix A that we are using: " << endl;
  A.Print();
  cout << endl;
  x = ConjugateGradient(A,b,x0);
  xsd = SteepestDescent(A,b,x0);
  cout << "The approximate solution using Conjugate Gradient is: " << endl;
  x.Print();
  cout << endl;
  cout << "The approximate solution using Steepest Descent is: " << endl;
  xsd.Print();
  cout << endl;
}
\end{minted}
% --- end paragraph admon ---



% !split
\subsection{The routine for the steepest descent method}

% --- begin paragraph admon ---
\paragraph{}
\begin{minted}[fontsize=\fontsize{9pt}{9pt},linenos=false,mathescape,baselinestretch=1.0,fontfamily=tt,xleftmargin=7mm]{c++}
Vector SteepestDescent(Matrix A, Vector b, Vector x0){
  int IterMax, i;
  int dim = x0.Dimension();
  const double tolerance = 1.0e-14;
  Vector x(dim),f(dim),z(dim);
  double c,alpha,d;
  IterMax = 30;
  x = x0;
  f = A*x-b;
  i = 0;
  while (i <= IterMax){
    z = A*f;
    c = dot(f,f);
    alpha = c/dot(f,z);
    x = x - alpha*f;
    f =  A*x-b;
    if(sqrt(dot(f,f)) < tolerance) break;
    i++;
  }
  return x;
} 
\end{minted}
% --- end paragraph admon ---




% !split
\subsection{Simple implementation of the Conjugate gradient algorithm}

% --- begin paragraph admon ---
\paragraph{}
\begin{minted}[fontsize=\fontsize{9pt}{9pt},linenos=false,mathescape,baselinestretch=1.0,fontfamily=tt,xleftmargin=7mm]{c++}
  Vector ConjugateGradient(Matrix A, Vector b, Vector x0){
  int dim = x0.Dimension();
  const double tolerance = 1.0e-14;
  Vector x(dim),r(dim),v(dim),z(dim);
  double c,t,d;

  x = x0;
  r = b - A*x;
  v = r;
  c = dot(r,r);
  int i = 0; IterMax = dim;
  while(i <= IterMax){
    z = A*v;
    t = c/dot(v,z);
    x = x + t*v;
    r = r - t*z;
    d = dot(r,r);
    if(sqrt(d) < tolerance)
      break;
    v = r + (d/c)*v;
    c = d;  i++;
  }
  return x;
} 
\end{minted}
% --- end paragraph admon ---




% !split
\subsection{Codes from numerical recipes}

% --- begin paragraph admon ---
\paragraph{}
You can however use codes we have adapted from the text \href{{http://www.nr.com/}}{Numerical Recipes in C++}, see chapter 10.7.  
Here we present a program, which you also can find at the webpage of the course we use the functions \textbf{dfpmin} and \textbf{lnsrch}.  

\begin{itemize}
\item The program uses the harmonic oscillator in one dimensions as example.

\item The program does not use armadillo to handle vectors and matrices, but employs rather my own vector-matrix class. These auxiliary functions, and the main program \emph{model.cpp} can all be found under the \href{{https://github.com/CompPhysics/ComputationalPhysics2/tree/gh-pages/doc/pub/cg/programs/c%2B%2B}}{program link here}.
\end{itemize}

\noindent
Below we show only excerpts from the main program. For the full program, see the above link.
% --- end paragraph admon ---




% !split
\subsection{Finding the minimum of the harmonic oscillator model in one dimension}

% --- begin paragraph admon ---
\paragraph{}
\begin{minted}[fontsize=\fontsize{9pt}{9pt},linenos=false,mathescape,baselinestretch=1.0,fontfamily=tt,xleftmargin=7mm]{c++}
//   Main function begins here
int main()
{
     int n, iter;
     double gtol, fret;
     double alpha;
     n = 1;
//   reserve space in memory for vectors containing the variational
//   parameters
     Vector g(n), p(n);
     cout << "Read in guess for alpha" << endl;
     cin >> alpha;
     gtol = 1.0e-5;
//   now call dfmin and compute the minimum
     p(0) = alpha;
     dfpmin(p, n, gtol, &iter, &fret, Efunction, dEfunction);
     cout << "Value of energy minimum = " << fret << endl;
     cout << "Number of iterations = " << iter << endl;
     cout << "Value of alpha at minimum = " << p(0) << endl;
      return 0;
}  // end of main program

\end{minted}
% --- end paragraph admon ---





% !split
\subsection{Functions to observe}

% --- begin paragraph admon ---
\paragraph{}
The functions \textbf{Efunction} and \textbf{dEfunction} compute the expectation value of the energy and its derivative.
These functions need to be changed when you want to your own derivatives.
\begin{minted}[fontsize=\fontsize{9pt}{9pt},linenos=false,mathescape,baselinestretch=1.0,fontfamily=tt,xleftmargin=7mm]{c++}
//  this function defines the expectation value of the local energy
double Efunction(Vector  &x)
{
  double value = x(0)*x(0)*0.5+1.0/(8*x(0)*x(0));
  return value;
} // end of function to evaluate

//  this function defines the derivative of the energy 
void dEfunction(Vector &x, Vector &g)
{
  g(0) = x(0)-1.0/(4*x(0)*x(0)*x(0));
} // end of function to evaluate
\end{minted}
You need to change these functions in order to compute the local energy for your system. I used 1000
cycles per call to get a new value of $\langle E_L[\alpha]\rangle$.
When I compute the local energy I also compute its derivative.
After roughly 10-20 iterations I got a converged result in terms of $\alpha$.
% --- end paragraph admon ---



% ------------------- end of main content ---------------


\printindex

\end{document}

