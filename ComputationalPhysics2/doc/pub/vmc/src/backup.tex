
\frame[containsverbatim]
{
  \frametitle{Importance sampling, what we want to do}
\begin{small}
{\scriptsize
We need to replace the brute force
Metropolis algorithm with a walk in coordinate space biased by the trial wave function.
This approach is based on the Fokker-Planck equation and the Langevin equation for generating a trajectory in coordinate space.  This is explained later.

For a diffusion process characterized by a time-dependent probability density $P(x,t)$ in one dimension the Fokker-Planck
equation reads (for one particle /walker) 
\[
   \frac{\partial P}{\partial t} = D\frac{\partial }{\partial x}\left(\frac{\partial }{\partial x} -F\right)P(x,t),
\]
where $F$ is a drift term and $D$ is the diffusion coefficient. 

The new positions in coordinate space are given as the solutions of the Langevin equation using Euler's method, namely,
we go from the Langevin equation
\[ 
   \frac{\partial x(t)}{\partial t} = DF(x(t)) +\eta,
\]
with $\eta$ a random variable,
yielding a new position 
\[
   y = x+DF(x)\Delta t +\xi,
\]
where $\xi$ is gaussian random variable and $\Delta t$ is a chosen time step. 
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, what we want to do}
\begin{small}
{\scriptsize
The process of isotropic diffusion characterized by a time-dependent probability density $P(\bfv{x},t)$ obeys (as an approximation) 
the so-called Fokker-Planck equation 
$$
   \frac{\partial P}{\partial t} = \sum_i D\frac{\partial }{\partial \bfv{x_i}}\left(\frac{\partial }{\partial \bfv{x_i}} -\bfv{F_i}\right)P(\bfv{x},t),
$$
where $\bfv{F_i}$ is the $i^{th}$ component of the drift term (drift velocity) caused by an external potential, and $D$ is the diffusion coefficient. The convergence to a stationary probability density can be obtained by setting the left hand side to zero. The resulting equation will be satisfied if and only if all the terms of the sum are equal zero,
$$
\frac{\partial^2 P}{\partial {\bfv{x_i}^2}} = P\frac{\partial}{\partial {\bfv{x_i}}}\bfv{F_i} + \bfv{F_i}\frac{\partial}{\partial {\bfv{x_i}}}P.
$$
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, what we want to do}
\begin{small}
{\scriptsize
The drift vector should be of the form $\bfv{F} = g(\bfv{x}) \frac{\partial P}{\partial \bfv{x}}$. Then,
$$
\frac{\partial^2 P}{\partial {\bfv{x_i}^2}} = P\frac{\partial g}{\partial P}\left( \frac{\partial P}{\partial {\bfv{x_i}}}  \right)^2 + P g \frac{\partial ^2 P}{\partial {\bfv{x_i} ^2}}  + g \left( \frac{\partial P}{\partial {\bfv{x_i}}}  \right)^2.
$$
The condition of stationary density means that the left hand side equals zero. In other words, the terms containing first and second derivatives have to cancel each other. It is possible only if $g = \frac{1}{P}$, which yields
\begin{equation}\label{quantumForceEQ}
\boxed{\bfv{F} = 2\frac{1}{\Psi_T}\nabla\Psi_T,}
\end{equation}
which is known as the so-called \emph{quantum force}. This term is responsible for pushing the walker towards regions of configuration space where the trial wave function is large, increasing the efficiency of the simulation in contrast to the Metropolis algorithm where the walker has the same probability of moving in every direction.
}
\end{small}
}





\frame
{
  \frametitle{Importance Sampling}
\begin{small}
{\scriptsize
The Fokker-Planck equation yields a (the solution to the equation) transition probability given by the Green's function
\[
  G(y,x,\Delta t) = \frac{1}{(4\pi D\Delta t)^{3N/2}} \exp{\left(-(y-x-D\Delta t F(x))^2/4D\Delta t\right)}
\]
which in turn means that our brute force Metropolis algorithm
\[ 
    A(y,x) = \mathrm{min}(1,q(y,x))),
\]
with $q(y,x) = |\Psi_T(y)|^2/|\Psi_T(x)|^2$ is now replaced by
\[
q(y,x) = \frac{G(x,y,\Delta t)|\Psi_T(y)|^2}{G(y,x,\Delta t)|\Psi_T(x)|^2}
\]
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, new positions}
%  \begin{block}{}
\begin{small}
{\scriptsize
\lstset{language=c++}  
\begin{lstlisting}
 for (variate=1; variate <= max_variations; variate++){
    // initialisations of variational parameters and energies 
    beta += 0.1;  
    energy = energy2 = delta_e = 0.0;
    //  initial trial position, note calling with beta 
    for (i = 0; i < number_particles; i++) { 
      for ( j=0; j < dimension; j++) {
	r_old[i][j] = gaussian_deviate(&idum)*sqrt(timestep);
      }
    }
    wfold = wave_function(r_old, beta);
    quantum_force(r_old, qforce_old, beta, wfold);
\end{lstlisting} 
}
\end{small}
%  \end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, new positions}
%  \begin{block}{}
\begin{small}
{\scriptsize
\lstset{language=c++}  
\begin{lstlisting}
    // loop over monte carlo cycles 
    for (cycles = 1; cycles <= number_cycles; cycles++){ 
      // new position 
      for (i = 0; i < number_particles; i++) { 
	for ( j=0; j < dimension; j++) {
	  // gaussian deviate to compute new positions using a given timestep
	  r_new[i][j] = r_old[i][j] + gaussian_deviate(&idum)*sqrt(timestep)+qforce_old[i][j]*timestep*D;
\end{lstlisting} 
}
\end{small}
%  \end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, new positions}
%  \begin{block}{}
\begin{small}
{\scriptsize
\lstset{language=c++}  
\begin{lstlisting}
//  we move only one particle  at the time
        for (k = 0; k < number_particles; k++) {
	  if ( k != i) {
	    for ( j=0; j < dimension; j++) {
	      r_new[k][j] = r_old[k][j];
	    }
	  } 
        }
	//        wave_function_onemove(r_new, qforce_new, &wfnew, beta); 
        wfnew = wave_function(r_new, beta); 
        quantum_force(r_new, qforce_new, beta, wfnew);
\end{lstlisting} 
}
\end{small}
%  \end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, new positions}
%  \begin{block}{}
\begin{small}
{\scriptsize
\lstset{language=c++}  
\begin{lstlisting}
	//  we compute the log of the ratio of the greens functions to be used in the 
	//  Metropolis-Hastings algorithm
        greensfunction = 0.0;            
	for ( j=0; j < dimension; j++) {
	  greensfunction += 0.5*(qforce_old[i][j]+qforce_new[i][j])*
	    (D*timestep*0.5*(qforce_old[i][j]-qforce_new[i][j])-r_new[i][j]+r_old[i][j]);
        }
        greensfunction = exp(greensfunction);
\end{lstlisting} 
}
\end{small}
%  \end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, new positions}
%  \begin{block}{}
\begin{small}
{\scriptsize
\lstset{language=c++}  
\begin{lstlisting}
 	// The Metropolis test is performed by moving one particle  at the time
	if(ran2(&idum) <= greensfunction*wfnew*wfnew/wfold/wfold ) { 
	  for ( j=0; j < dimension; j++) {
	    r_old[i][j] = r_new[i][j];
	    qforce_old[i][j] = qforce_new[i][j];
	  }
	  wfold = wfnew;
          .....
\end{lstlisting} 
}
\end{small}
%  \end{block}
}



\frame[containsverbatim]
{
  \frametitle{Importance sampling, Quantum force}
%  \begin{block}{}
\begin{small}
{\scriptsize
\lstset{language=c++}  
\begin{lstlisting}
void  quantum_force(double **r, double **qforce, double beta, double wf)
{
  int i, j;
  double wfminus, wfplus; 
  double **r_plus, **r_minus;

  r_plus = (double **) matrix( number_particles, dimension, sizeof(double));
  r_minus = (double **) matrix( number_particles, dimension, sizeof(double));
  for (i = 0; i < number_particles; i++) { 
    for ( j=0; j < dimension; j++) {
      r_plus[i][j] = r_minus[i][j] = r[i][j];
    }
  }
...
\end{lstlisting} 
}
\end{small}
%  \end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Quantum force in function vmc\_importance.cpp, brute force derivative}
%  \begin{block}{}
\begin{small}
{\scriptsize
\lstset{language=c++}  
\begin{lstlisting}

  // compute the first derivative
  for (i = 0; i < number_particles; i++) {
    for (j = 0; j < dimension; j++) { 
      r_plus[i][j] = r[i][j]+h;
      r_minus[i][j] = r[i][j]-h;
      wfminus = wave_function(r_minus, beta); 
      wfplus  = wave_function(r_plus, beta); 
      qforce[i][j] = (wfplus-wfminus)*2.0/wf/(2*h);
      r_plus[i][j] = r[i][j];
      r_minus[i][j] = r[i][j];
    }
  }

} // end of quantum_force function
\end{lstlisting} 
}
\end{small}
%  \end{block}
}

\frame
{
  \frametitle{Closed form expressions for quantum force}
\begin{small}
{\scriptsize
The general derivative formula of the Jastrow factor is
\[
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k}
+
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_k}
\]
However, 
with our
\[
\Psi_C=\prod_{i< j}g(r_{ij})= \exp{\left\{\sum_{i<j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
\]
the gradient needed for the quantum force and local energy is easy to compute.  
We get for particle  $k$
\[
\frac{ \nabla_k \Psi_C}{ \Psi_C }= \sum_{j\ne k}\frac{{\bf r}_{kj}}{r_{kj}}\frac{a}{(1+\beta r_{kj})^2},
\]
which is rather easy to code.  Remember to sum over all particles  when you compute the local energy.
}
\end{small}
}

\frame
{
  \frametitle{Your tasks from the previous  weeks plus new tasks}
\begin{small}
{\scriptsize
\begin{itemize}
\item Implement the closed form expression for the local energy and the so-called quantum force
\item Convince yourself that the closed form expressions are correct.
\item Implement the closed form expressions for systems with more than two electrons.
\item Start implementing importance sampling, part 1c
\item You need to produce random numbers with a Gaussian distribution.
\item Reading task: Thijssen's text chapters 8.8 and 12.2.
\end{itemize}
}
\end{small}
}

\frame
{
  \frametitle{Structuring the code, simple task}
\begin{small}
{\scriptsize
\begin{itemize}
\item Make another copy of your code.
\item Implement the closed form expression for the local energy
\item Compile the new and old codes with the -pg option for profiling.
\item Run both codes and profile them afterwards using $\mathrm{gprof} \{\mathrm{name executable}\} > \mathrm{outprofile}$
\item Study the time usage in the file {\bf outprofile}
\end{itemize}
}
\end{small}
}


\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

In the Metropolis/Hasting algorithm, the \emph{acceptance ratio} determines the probability for a particle  to be accepted at a new position. The ratio of the trial wave functions evaluated at the new and current positions is given by

\begin{equation}\label{acceptanceRatio}
\boxed{R \equiv \frac{\Psi_{T}^{new}}{\Psi_{T}^{cur}} = \underbrace{\frac{\Psi_{D}^{new}}{\Psi_{D}^{cur}}}_{R_{SD}}\, \underbrace{\frac{\Psi_{C}^{new}}{\Psi_{C}^{cur}}}_{R_{C}}.}
\end{equation}
Here $\Psi_{D}$ is our Slater determinant while $\Psi_{C}$ is our correlation function. 
We need to optimize $\nabla \Psi_T / \Psi_T$ ratio and the second derivative as well, that is
the $\nabla^2 \Psi_T/\Psi_T$ ratio. The first is needed when we compute the so-called quantum force in importance sampling.
The second is needed when we compute the kinetic energy term of the local energy.
\[
\frac{\Grad \Psi}{\Psi}  = \frac{\Grad (\Psi_{D} \, \Psi_{C})}{\Psi_{D} \, \Psi_{C}}  =  \frac{ \Psi_C \Grad \Psi_{D} + \Psi_{D} \Grad \Psi_{C}}{\Psi_{D} \Psi_{C}} = \frac{\Grad \Psi_{D}}{\Psi_{D}} + \frac{\Grad  \Psi_C}{ \Psi_C}
\]
 }
 \end{small}
 }


\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

The expectation value of the kinetic energy expressed in atomic units for electron $i$ is 
\begin{equation}
 \langle \Op{K}_i \rangle = -\frac{1}{2}\frac{\langle\Psi|\nabla_{i}^2|\Psi \rangle}{\langle\Psi|\Psi \rangle},
\end{equation}

\begin{equation}\label{kineticE}
K_i = -\frac{1}{2}\frac{\nabla_{i}^{2} \Psi}{\Psi}.
\end{equation}
\begin{eqnarray}
\frac{\nabla^2 \Psi}{\Psi} & = & \frac{\nabla^2 ({\Psi_{D} \,  \Psi_C})}{\Psi_{D} \,  \Psi_C} = \frac{\Grad \cdot [\Grad {(\Psi_{D} \,  \Psi_C)}]}{\Psi_{D} \,  \Psi_C} = \frac{\Grad \cdot [ \Psi_C \Grad \Psi_{D} + \Psi_{D} \Grad  \Psi_C]}{\Psi_{D} \,  \Psi_C}\nonumber\\
&  = & \frac{\Grad  \Psi_C \cdot \Grad \Psi_{D} +  \Psi_C \nabla^2 \Psi_{D} + \Grad \Psi_{D} \cdot \Grad  \Psi_C + \Psi_{D} \nabla^2  \Psi_C}{\Psi_{D} \,  \Psi_C}\nonumber\\
\end{eqnarray}
\begin{eqnarray}
\frac{\nabla^2 \Psi}{\Psi}
& = & \frac{\nabla^2 \Psi_{D}}{\Psi_{D}} + \frac{\nabla^2  \Psi_C}{ \Psi_C} + 2 \frac{\Grad \Psi_{D}}{\Psi_{D}}\cdot\frac{\Grad  \Psi_C}{ \Psi_C}
\end{eqnarray}
 }
 \end{small}
 }


\frame
 {
   \frametitle{Definitions}
 \begin{small}
 {\scriptsize
We define the correlated function as
\[
\Psi_C=\prod_{i< j}g(r_{ij})=\prod_{i< j}^Ng(r_{ij})= \prod_{i=1}^N\prod_{j=i+1}^Ng(r_{ij}),
\]
with 
$r_{ij}=|{\bf r}_i-{\bf r}_j|=\sqrt{(x_i-x_j)^2+(y_i-y_j)^2+(z_i-z_j)^2}$ for three dimensions and
$r_{ij}=|{\bf r}_i-{\bf r}_j|=\sqrt{(x_i-x_j)^2+(y_i-y_j)^2}$ for two dimensions.

In our particular case we have
\[
\Psi_C=\prod_{i< j}g(r_{ij})=\exp{\left\{\sum_{i<j}f(r_{ij})\right\}}=
\exp{\left\{\sum_{i<j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
\]
 }
 \end{small}
 }





\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

The total number of different relative distances $r_{ij}$ is $N(N-1)/2$. In a matrix storage format, the set forms a strictly upper triangular matrix
\begin{equation}\label{utrij}
 \bfv{r} \equiv \begin{pmatrix}
  0 & r_{1,2} & r_{1,3} & \cdots & r_{1,N} \\
  \vdots & 0       & r_{2,3} & \cdots & r_{2,N} \\
  \vdots & \vdots  & 0  & \ddots & \vdots  \\
  \vdots & \vdots  & \vdots  & \ddots  & r_{N-1,N} \\
  0 & 0  & 0  & \cdots  & 0
 \end{pmatrix}.
\end{equation}
This applies to  $\bfv{g} = \bfv{g}(r_{ij})$ as well. 

In our algorithm we will move one particle  at the time, say the $kth$-particle . Keep this in mind in the discussion to come.
 }
 \end{small}
 }


\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize
\begin{equation}\label{RjfRatio}
 \boxed{R_{C} = \frac{\Psi_{C}^\mathrm{new}}{\Psi_{C}^\mathrm{cur}} =
\prod_{i=1}^{k-1}\frac{g_{ik}^\mathrm{new}}{g_{ik}^\mathrm{cur}}\;
\prod_{i=k+1}^{N}\frac{g_{ki}^\mathrm{new}}{g_{ki}^\mathrm{cur}}}.
\end{equation}\label{padepadeRatio}
For the Pad\'e-Jastrow form
\begin{equation}
 \boxed{R_{C} = \frac{\Psi_{C}^\mathrm{new}}{\Psi_{C}^\mathrm{cur}} = \frac{e^{U_{new}}}{e^{U_{cur}}} = e^{\Delta U},}
\end{equation}
where
\begin{equation}
\Delta U =
\sum_{i=1}^{k-1}\big(f_{ik}^\mathrm{new}-f_{ik}^\mathrm{cur}\big)
+
\sum_{i=k+1}^{N}\big(f_{ki}^\mathrm{new}-f_{ki}^\mathrm{cur}\big)
\end{equation}

One needs to develop a special algorithm 
that runs only through the elements of the upper triangular
matrix $\bfv{g}$ and have $k$ as an index. 

 }
 \end{small}
 }


\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize
The expression to be derived in the following is of interest when computing the quantum force and the kinetic energy. It has the form
$$
\frac{\bfv{{\nabla_i}}\Psi_C}{\Psi_C} = \frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_i},
$$
for all dimensions and with $i$ running over all particles.
For the first derivative only $N-1$ terms survive the ratio because the $g$-terms that are not differentiated cancel with their corresponding ones in the denominator. Then,
\begin{equation}\label{1jgradG}
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{1}{g_{ik}}\frac{\partial g_{ik}}{\partial x_k}
+
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\frac{\partial g_{ki}}{\partial x_k}.
\end{equation}
An equivalent equation is obtained for the exponential form after replacing $g_{ij}$ by $\exp(f_{ij})$, yielding:
\begin{equation}\label{1jgradEG}
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k}
+
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_k},
\end{equation}
with both expressions scaling as $\mathcal{O}(N)$.
 }
 \end{small}
 }


\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize
Using the identity 
\begin{equation}\label{firstDerIdentity}
\frac{\partial}{\partial x_i}g_{ij} = -\frac{\partial}{\partial x_j}g_{ij} 
\end{equation}
on the right hand side terms of Eq.~(\ref{1jgradG}) and Eq.~(\ref{1jgradEG}), we get expressions where all the derivatives acting on the particle  are represented by the
\emph{second} index of $g$:
\begin{equation}\label{gradJasGen}
\boxed{
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{1}{g_{ik}}\frac{\partial g_{ik}}{\partial x_k}
-
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\frac{\partial g_{ki}}{\partial x_i},
}
\end{equation}
and for the exponential case:
\begin{equation}\label{gradJasGenExp}
\boxed{
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k}
-
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_i}.
}
\end{equation}
 }
 \end{small}
 }
\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

For correlation forms depending only on the scalar distances $r_{ij}$ we can use the chain rule. Noting that 
\begin{equation}\label{chainRule}
\frac{\partial g_{ij}}{\partial x_j} = \frac{\partial g_{ij}}{\partial r_{ij}} \frac{\partial r_{ij}}{\partial x_j} = \frac{x_j - x_i}{r_{ij}} \frac{\partial g_{ij}}{\partial r_{ij}},
\end{equation}
after substitution in Eq.~(\ref{gradJasGen}) and Eq.~(\ref{gradJasGenExp}) we arrive at
\begin{equation}\label{generalCorrelation}
\boxed{
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} = 
\sum_{i=1}^{k-1}\frac{1}{g_{ik}} \frac{\bfv{r_{ik}}}{r_{ik}} \frac{\partial g_{ik}}{\partial r_{ik}}
-
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\frac{\bfv{r_{ki}}}{r_{ki}}\frac{\partial g_{ki}}{\partial r_{ki}}.
}
\end{equation}
 }
 \end{small}
 }
\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

Note that for the Pad\'e-Jastrow form we can set $g_{ij} \equiv g(r_{ij}) = e^{f(r_{ij})} = e^{f_{ij}}$ and 
\begin{equation}
\frac{\partial g_{ij}}{\partial r_{ij}} = g_{ij} \frac{\partial f_{ij}}{\partial r_{ij}}.
\end{equation}
Therefore, 
\begin{equation}\label{padeJastrowGradJasRatio}
\boxed{
\frac{1}{\Psi_{C}}\frac{\partial \Psi_{C}}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{\bfv{r_{ik}}}{r_{ik}}\frac{\partial f_{ik}}{\partial r_{ik}}
-
\sum_{i=k+1}^{N}\frac{\bfv{r_{ki}}}{r_{ki}}\frac{\partial f_{ki}}{\partial r_{ki}},
}
\end{equation}
where 
\begin{equation}\label{distanceVector}
 \bfv{r}_{ij} = |\bfv{r}_j - \bfv{r}_i| = (x_j - x_i)\vec{e}_1 + (y_j - y_i)\vec{e}_2 + (z_j - z_i)\vec{e}_3
\end{equation}
is the vectorial distance. When the correlation function is the \emph{linear Pad\'e-Jastrow} we set \begin{equation}
f_{ij} = \frac{a r_{ij}}{(1 + \beta r_{ij})},
\end{equation}
which yields the closed form expression
\begin{equation}\label{analyticalPJGrad}
 \boxed{\frac{\partial f_{ij}}{\partial r_{ij}} = \frac{a}{(1 + \beta r_{ij})^2}}.
\end{equation}
 }
 \end{small}
 }



\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

{Computing the $\nabla^2 \Psi_C/\Psi_C$ ratio}

\[\bfv{\nabla}_k \Psi_C = 
\sum_{i=1}^{k-1}\frac{1}{g_{ik}} \bfv{\nabla}_k g_{ik}
+
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\bfv{\nabla}_k g_{ki}.\]
After multiplying by $\Psi_C$ and taking the gradient on both sides we get,
\begin{align}\label{gradLap}
\nabla_{k}^2 \Psi_C & = \bfv{\nabla}_k \Psi_C \cdot 
\left(\sum_{i=1}^{k-1}\frac{1}{g_{ik}} \bfv{\nabla}_k g_{ik}
+
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\bfv{\nabla}_k g_{ki}\right)\nonumber\\
&+
\Psi_C \nabla_k \cdot \left(\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\bfv{\nabla}_k g_{ki}
+
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\bfv{\nabla}_k g_{ki}\right)\nonumber\\
& = \Psi_C \left(\frac{\bfv{\nabla}_k \Psi_C}{\Psi_C}\right)^2 +
\Psi_C \nabla_k \cdot \left(\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\bfv{\nabla}_k g_{ki}
+
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\bfv{\nabla}_k g_{ki}\right).
\end{align}
 }
 \end{small}
 }
\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

Now,
\begin{align}
 \bfv{\nabla}_k \cdot \left(\frac{1}{g_{ik}}\bfv{\nabla}_k g_{ik}\right) &= \bfv{\nabla}_k \left(\frac{1}{g_{ik}}\right)\cdot \bfv{\nabla}_k g_{ik} + \frac{1}{g_{ik}}\bfv{\nabla}_k \cdot \bfv{\nabla}_k g_{ik}\nonumber\\
 & = -\frac{1}{g_{ik}^2} \bfv{\nabla}_k g_{ik} \cdot \bfv{\nabla}_k g_{ik} + \frac{1}{g_{ik}} \bfv{\nabla}_k \cdot \left(\frac{\bfv{r}_{ik}}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right)\nonumber\\
 & = -\frac{1}{g_{ik}^2} (\bfv{\nabla}_k g_{ik})^2 \nonumber\\&+ \frac{1}{g_{ik}}\left[\bfv{\nabla}_k \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right)\cdot \bfv{r}_{ik} + \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right) \bfv{\nabla}_k \cdot \bfv{r}_{ik}  \right] \nonumber\\
 &= -\frac{1}{g_{ik}^2} \left(\frac{\bfv{r}_{ik}}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right)^2\nonumber\\ &+ \frac{1}{g_{ik}}\left[\bfv{\nabla}_k \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right)\cdot \bfv{r}_{ik} + \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right) d  \right]\nonumber\\
 &= -\frac{1}{g_{ik}^2} \left(\frac{\partial g_{ik}}{\partial r_{ik}}\right)^2\nonumber\\ &+ \frac{1}{g_{ik}}\left[\bfv{\nabla}_k \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right)\cdot \bfv{r}_{ik} + \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right) d  \right], \label{subs0}
 \end{align}
with $d$ being the number of spatial dimensions.
 }
 \end{small}
 }
\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

Moreover, 
\begin{align*}
\bfv{\nabla}_k \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right) &= \frac{\bfv{r}_{ik}}{r_{ik}} \frac{\partial }{\partial r_{ik}} \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right)\nonumber\\
&=\frac{\bfv{r}_{ik}}{r_{ik}}\left(-\frac{1}{r_{ik}^2}\frac{\partial g_{ik}}{\partial r_{ik}} + \frac{1}{r_{ik}}\frac{\partial^2 g_{ik}}{\partial r_{ik}^2}\right).\label{subs1}
\end{align*}

We finally get
\begin{align*}
  \bfv{\nabla}_k \cdot \left(\frac{1}{g_{ik}}\bfv{\nabla}_k g_{ik}\right) &= -\frac{1}{g_{ik}^2}\left(\frac{\partial g_{ik}}{\partial r_{ik}}\right)^2 + \frac{1}{g_{ik}}\left[\left(\frac{d-1}{r_{ik}}\right)\frac{\partial g_{ik}}{\partial r_{ik}} + \frac{\partial^2 g_{ik}}{\partial r_{ik}^2} \right].
\end{align*}
 }
 \end{small}
 }
\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

Inserting the last expression in Eq.~(\ref{gradLap}) and after division by $\Psi_C$ we get,

\begin{align}
 \frac{\nabla_{k}^2 \Psi_C}{\Psi_C} & =  \left(\frac{\bfv{\nabla}_k \Psi_C}{\Psi_C}\right)^2 \nonumber\\
 & + \sum_{i=1}^{k-1} -\frac{1}{g_{ik}^2}\left(\frac{\partial g_{ik}}{\partial r_{ik}}\right)^2 + \frac{1}{g_{ik}}\left[\left(\frac{d-1}{r_{ik}}\right)\frac{\partial g_{ik}}{\partial r_{ik}} + \frac{\partial^2 g_{ik}}{\partial r_{ik}^2} \right]\nonumber\\
 & + \sum_{i=k+1}^{N} -\frac{1}{g_{ki}^2}\left(\frac{\partial g_{ki}}{\partial r_{ki}}\right)^2 + \frac{1}{g_{ki}}\left[\left(\frac{d-1}{r_{ki}}\right)\frac{\partial g_{ki}}{\partial r_{ki}} + \frac{\partial^2 g_{ki}}{\partial r_{ki}^2} \right].
\end{align}
 }
 \end{small}
 }
\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

For the exponential case we have
\begin{align*}
 \frac{\nabla_{k}^2 \Psi_{C}}{\Psi_{C}} & =  \left(\frac{\bfv{\nabla}_k \Psi_{C}}{\Psi_{C}}\right)^2 \nonumber\\
 & + \sum_{i=1}^{k-1} -\frac{1}{g_{ik}^2}\left(g_{ik}\frac{\partial f_{ik}}{\partial r_{ik}}\right)^2 + \frac{1}{g_{ik}}\left[\left(\frac{d-1}{r_{ik}}\right)g_{ik}\frac{\partial f_{ik}}{\partial r_{ik}} + \frac{\partial }{\partial r_{ik}}\left(g_{ik}\frac{\partial f_{ik}}{\partial r_{ik}}\right) \right]\nonumber\\
 & + \sum_{i=k+1}^{N} -\frac{1}{g_{ki}^2}\left(g_{ik}\frac{\partial f_{ki}}{\partial r_{ki}}\right)^2 + \frac{1}{g_{ki}}\left[\left(\frac{d-1}{r_{ki}}\right)g_{ki}\frac{\partial f_{ki}}{\partial r_{ki}} + \frac{\partial }{\partial r_{ki}}\left(g_{ki}\frac{\partial f_{ki}}{\partial r_{ki}}\right) \right].
 \end{align*}
 }
 \end{small}
 }\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

Using
\begin{align*}
 \frac{\partial }{\partial r_{ik}}\left(g_{ik}\frac{\partial f_{ik}}{\partial r_{ik}}\right) & = \frac{\partial g_{ik}}{\partial r_{ik}}\frac{\partial f_{ik}}{\partial r_{ik}} + g_{ik}\frac{\partial^2 f_{ik}}{\partial r_{ik}^2}\\
 & = g_{ik}\frac{\partial f_{ik}}{\partial r_{ik}}\frac{\partial f_{ik}}{\partial r_{ik}} + g_{ik}\frac{\partial^2 f_{ik}}{\partial r_{ik}^2}\\
 & = g_{ik}\left(\frac{\partial f_{ik}}{\partial r_{ik}}\right)^2 + g_{ik}\frac{\partial^2 f_{ik}}{\partial r_{ik}^2}
\end{align*}
and substituting this result into the equation above gives rise to the final expression,
\begin{align}\label{lapJasRatio}
\frac{\nabla_{k}^2 \Psi_{PJ}}{\Psi_{PJ}}  &=  \left(\frac{\bfv{\nabla}_k \Psi_{PJ}}{\Psi_{PJ}}\right)^2\nonumber\\
  &+ \sum_{i=1}^{k-1} \left[\left(\frac{d-1}{r_{ik}}\right)\frac{\partial f_{ik}}{\partial r_{ik}} + \frac{\partial^2  f_{ik}}{\partial r_{ik}^2} \right]
  + \sum_{i=k+1}^{N} \left[\left(\frac{d-1}{r_{ki}}\right)\frac{\partial f_{ki}}{\partial r_{ki}} + \frac{\partial^2 f_{ki}}{\partial r_{ki}^2} \right].
 \end{align}
 }
 \end{small}
 }


\frame
{
  \frametitle{Summing up: Bringing it all together, Local energy}
\begin{small}
{\scriptsize

The second derivative of the Jastrow factor divided by the Jastrow factor (the way it enters the kinetic energy) is
\[
\left[\frac{\nabla^2 \Psi_C}{\Psi_C}\right]_x =\  
2\sum_{k=1}^{N}
\sum_{i=1}^{k-1}\frac{\partial^2 g_{ik}}{\partial x_k^2}\ +\ 
\sum_{k=1}^N
\left(
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k} -
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_i}
\right)^2
\]
But we have a simple form for the function, namely
\[
\Psi_{C}=\prod_{i< j}\exp{f(r_{ij})}= \exp{\left\{\sum_{i<j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
\]
and it is easy to see that for particle  $k$
we have
\[
  \frac{\nabla^2_k \Psi_C}{\Psi_C }=
\sum_{ij\ne k}\frac{({\bf r}_k-{\bf r}_i)({\bf r}_k-{\bf r}_j)}{r_{ki}r_{kj}}f'(r_{ki})f'(r_{kj})+
\sum_{j\ne k}\left( f''(r_{kj})+\frac{2}{r_{kj}}f'(r_{kj})\right)
\]
}
\end{small}
}



\frame
{
  \frametitle{Bringing it all together, Local energy}
\begin{small}
{\scriptsize
Using 
\[
f(r_{ij})= \frac{ar_{ij}}{1+\beta r_{ij}},
\]
and $g'(r_{kj})=dg(r_{kj})/dr_{kj}$ and 
$g''(r_{kj})=d^2g(r_{kj})/dr_{kj}^2$  we find that for particle  $k$
we have
\[
  \frac{\nabla^2_k \Psi_C}{\Psi_C }=
\sum_{ij\ne k}\frac{({\bf r}_k-{\bf r}_i)({\bf r}_k-{\bf r}_j)}{r_{ki}r_{kj}}\frac{a}{(1+\beta r_{ki})^2}
\frac{a}{(1+\beta r_{kj})^2}+
\sum_{j\ne k}\left(\frac{2a}{r_{kj}(1+\beta r_{kj})^2}-\frac{2a\beta}{(1+\beta r_{kj})^3}\right)
\]
}
\end{small}
}

\frame
{
  \frametitle{Important feature}
\begin{small}
{\scriptsize
For the correlation part 
\[
\Psi_C=\prod_{i< j}g(r_{ij})= \exp{\left\{\sum_{i<j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
\]
we need to take into account whether electrons have equal or opposite spins since we have to obey the
electron-electron cusp condition as well.  
When the electrons have  equal spins 
\[
a= 1/4,
\]
while for opposite spins (like the ground state in Helium)
\[
a= 1/2
\] 
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equations}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
A stochastic process is simply a function of two variables, one is the time,
the other is a stochastic variable $X$, defined by specifying
\begin{itemize}
   \item the set $\left\{x\right\}$ of possible values for $X$;
   \item  the probability distribution, $w_X(x)$, 
over this set, or briefly $w(x)$
\end{itemize}
The set of values $\left\{x\right\}$ for $X$ 
may be discrete, or continuous. If the set of
values is continuous, then $w_X (x)$ is a probability density so that 
$w_X (x)dx$
is the probability that one finds the stochastic variable $X$ to have values
in the range $[x, x + dx]$ .
}
\end{small}
%\end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equations}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
     An arbitrary number of other stochastic variables may be derived from
$X$. For example, any $Y$ given by a mapping of $X$, is also a stochastic
variable. The mapping may also be time-dependent, that is, the mapping
depends on an additional variable $t$
\[
                              Y_X (t) = f (X, t) .
\]
The quantity $Y_X (t)$ is called a random function, or, since $t$ often is time,
a stochastic process. A stochastic process is a function of two variables,
one is the time, the other is a stochastic variable $X$. Let $x$ be one of the
possible values of $X$ then\[
                               y(t) = f (x, t),\]
is a function of $t$, called a sample function or realization of the process.
In physics one considers the stochastic process to be an ensemble of such
sample functions.
}
\end{small}
  %\end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equations}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
     For many physical systems initial distributions of a stochastic 
variable $y$ tend to equilibrium distributions: $w(y, t)\rightarrow w_0(y)$ 
as $t\rightarrow\infty$. In
equilibrium detailed balance constrains the transition rates
\[
     W(y\rightarrow y')w(y ) = W(y'\rightarrow y)w_0 (y),
\]
where $W(y'\rightarrow y)$ 
is the probability, per unit time, that the system changes
from a state $|y\rangle$ , characterized by the value $y$ 
for the stochastic variable $Y$ , to a state $|y'\rangle$.

Note that for a system in equilibrium the transition rate 
$W(y'\rightarrow y)$ and
the reverse $W(y\rightarrow y')$ may be very different. 
}
\end{small}
  %\end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equations}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
Consider, for instance, a simple
system that has only two energy levels $\epsilon_0 = 0$ and 
$\epsilon_1 = \Delta E$. 

For a system governed by the Boltzmann distribution we find (the partition function has been taken out)
\[
     W(0\rightarrow 1)\exp{-\epsilon_0/kT} = W(1\rightarrow 0)\exp{-\epsilon_1/kT}
\]
We get then
\[
     \frac{W(1\rightarrow 0)}{W(0 \rightarrow 1)}=\exp{-\Delta E/kT},
\]
which goes to zero when $T$ tends to zero.
}
\end{small}
  %\end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equations}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
If we assume a discrete set events,
our initial probability
distribution function can be  given by 
\[
   w_i(0) = \delta_{i,0},
\]
and its time-development after a given time step $\Delta t=\epsilon$ is
\[ 
   w_i(t) = \sum_{j}W(j\rightarrow i)w_j(t=0).
\]   
The continuous analog to $w_i(0)$ is
\be
   w({\bf x})\rightarrow \delta({\bf x}),
\ee
where we now have generalized the one-dimensional position $x$ to a generic-dimensional  
vector ${\bf x}$. The Kroenecker $\delta$ function is replaced by the $\delta$ distribution
function $\delta({\bf x})$ at  $t=0$.  
}
\end{small}
  %\end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equations}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
The transition from a state $j$ to a state $i$ is now replaced by a transition
to a state with position ${\bf y}$ from a state with position ${\bf x}$. 
The discrete sum of transition probabilities can then be replaced by an integral
and we obtain the new distribution at a time $t+\Delta t$ as 
\be
   w({\bf y},t+\Delta t)= \int W({\bf y},t+\Delta t| {\bf x},t)w({\bf x},t)d{\bf x},
\ee
and after $m$ time steps we have
\be
   w({\bf y},t+m\Delta t)= \int W({\bf y},t+m\Delta t| {\bf x},t)w({\bf x},t)d{\bf x}.
\ee
When equilibrium is reached we have
\be
   w({\bf y})= \int W({\bf y}|{\bf x}, t)w({\bf x})d{\bf x},
\ee
that is no time-dependence. Note our change of notation for $W$
}
\end{small}
  %\end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equations}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
We can solve the equation for $w({\bf y},t)$ by making a Fourier transform to
momentum space. 
The PDF $w({\bf x},t)$ is related to its Fourier transform
$\tilde{w}({\bf k},t)$ through
\be\label{eq:fouriertransform}
   w({\bf x},t) = \int_{-\infty}^{\infty}d{\bf k} \exp{(i{\bf kx})}\tilde{w}({\bf k},t),
\ee
and using the definition of the 
$\delta$-function 
\be
   \delta({\bf x}) = \frac{1}{2\pi} \int_{-\infty}^{\infty}d{\bf k} \exp{(i{\bf kx})},
\ee
 we see that
\be
   \tilde{w}({\bf k},0)=1/2\pi.
\ee
}
\end{small}
  %\end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equations}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
We can then use the Fourier-transformed diffusion equation 
\begin{equation}
    \frac{\partial \tilde{w}({\bf k},t)}{\partial t} = -D{\bf k}^2\tilde{w}({\bf k},t),
\end{equation}
with the obvious solution
\begin{equation}
   \tilde{w}({\bf k},t)=\tilde{w}({\bf k},0)\exp{\left[-(D{\bf k}^2t)\right)}=
    \frac{1}{2\pi}\exp{\left[-(D{\bf k}^2t)\right]}. 
\end{equation}
}
\end{small}
  %\end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equations}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
Using Eq.~(\ref{eq:fouriertransform}) we obtain 
\begin{equation}\label{eq:finalw}
   w({\bf x},t)=\int_{-\infty}^{\infty}d{\bf k} \exp{\left[i{\bf kx}\right]}\frac{1}{2\pi}\exp{\left[-(D{\bf k}^2t)\right]}=
    \frac{1}{\sqrt{4\pi Dt}}\exp{\left[-({\bf x}^2/4Dt)\right]}, 
\end{equation}
with the normalization condition
\be
   \int_{-\infty}^{\infty}w({\bf x},t)d{\bf x}=1.
\ee
}
\end{small}
  %\end{block}
}
\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equations}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
It is rather easy to verify by insertion that Eq.~(\ref{eq:finalw}) is a solution
of the diffusion equation. The solution represents the probability of finding
our random walker at position ${\bf x}$ at time $t$ if the initial distribution 
was placed at ${\bf x}=0$ at $t=0$. 

There is another interesting feature worth observing. The discrete transition probability $W$
itself is given by a binomial distribution.
The results from the central limit theorem state that 
transition probability in the limit $n\rightarrow \infty$ converges to the normal 
distribution. It is then possible to show that
\be 
    W(il-jl,n\epsilon)\rightarrow W({\bf y},t+\Delta t|{\bf x},t)=
    \frac{1}{\sqrt{4\pi D\Delta t}}\exp{\left[-(({\bf y}-{\bf x})^2/4D\Delta t)\right]},
\ee
and that it satisfies the normalization condition and is itself a solution
to the diffusion equation.
}
\end{small}
  %\end{block}
}
\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equations}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
Let us now assume that we have three PDFs for times $t_0 < t' < t$, that is
$w({\bf x}_0,t_0)$, $w({\bf x}',t')$ and $w({\bf x},t)$.
We have then  
\[
   w({\bf x},t)= \int_{-\infty}^{\infty} W({\bf x}.t|{\bf x}'.t')w({\bf x}',t')d{\bf x}',
\]
and
\[
   w({\bf x},t)= \int_{-\infty}^{\infty} W({\bf x}.t|{\bf x}_0.t_0)w({\bf x}_0,t_0)d{\bf x}_0,
\]
and
\[
   w({\bf x}',t')= \int_{-\infty}^{\infty} W({\bf x}'.t'|{\bf x}_0,t_0)w({\bf x}_0,t_0)d{\bf x}_0.
\]
}
\end{small}
  %\end{block}
}
\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equations}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
We can combine these equations and arrive at the famous Einstein-Smoluchenski-Kolmogorov-Chapman (ESKC) relation
\[
 W({\bf x}t|{\bf x}_0t_0)  = \int_{-\infty}^{\infty} W({\bf x},t|{\bf x}',t')W({\bf x}',t'|{\bf x}_0,t_0)d{\bf x}'.
\]
We can replace the spatial dependence with a dependence upon say the velocity
(or momentum), that is we have
\[
 W({\bf v},t|{\bf v}_0,t_0)  = \int_{-\infty}^{\infty} W({\bf v},t|{\bf v}',t')W({\bf v}',t'|{\bf v}_0,t_0)d{\bf x}'.
\]
}
\end{small}
  %\end{block}
}

\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equations}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
We will now derive the Fokker-Planck equation. 
We start from the ESKC equation
\[
 W({\bf x},t|{\bf x}_0,t_0)  = \int_{-\infty}^{\infty} W({\bf x},t|{\bf x}',t')W({\bf x}',t'|{\bf x}_0,t_0)d{\bf x}'.
\]
Define $s=t'-t_0$, $\tau=t-t'$ and $t-t_0=s+\tau$. We have then
\[
 W({\bf x},s+\tau|{\bf x}_0)  = \int_{-\infty}^{\infty} W({\bf x},\tau|{\bf x}')W({\bf x}',s|{\bf x}_0)d{\bf x}'.
\]
}
\end{small}
  %\end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equations}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
Assume now that $\tau$ is very small so that we can make an expansion in terms of a small step $xi$, with ${\bf x}'={\bf x}-\xi$, that is
\[
 W({\bf x},s|{\bf x}_0)+\frac{\partial W}{\partial s}\tau +O(\tau^2) = \int_{-\infty}^{\infty} W({\bf x},\tau|{\bf x}-\xi)W({\bf x}-\xi,s|{\bf x}_0)d{\bf x}'.
\]
We assume that $W({\bf x},\tau|{\bf x}-\xi)$ takes non-negligible values only when $\xi$ is small. This is just another way of stating the Master equation!!
}
\end{small}
  %\end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equations}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
We say thus that ${\bf x}$ changes only by a small amount in the time interval $\tau$. 
This means that we can make a Taylor expansion in terms of $\xi$, that is we
expand
\[
W({\bf x},\tau|{\bf x}-\xi)W({\bf x}-\xi,s|{\bf x}_0) =
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}\left[W({\bf x}+\xi,\tau|{\bf x})W({\bf x},s|{\bf x}_0)
\right].
\]
We can then rewrite the ESKC equation as 
\[
\frac{\partial W}{\partial s}\tau=-W({\bf x},s|{\bf x}_0)+
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}
\left[W({\bf x},s|{\bf x}_0)\int_{-\infty}^{\infty} \xi^nW({\bf x}+\xi,\tau|{\bf x})d\xi\right].
\]
We have neglected higher powers of $\tau$ and have used that for $n=0$ 
we get simply $W({\bf x},s|{\bf x}_0)$ due to normalization.
}
\end{small}
  %\end{block}
}

\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equations}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
We say thus that ${\bf x}$ changes only by a small amount in the time interval $\tau$. 
This means that we can make a Taylor expansion in terms of $\xi$, that is we
expand
\[
W({\bf x},\tau|{\bf x}-\xi)W({\bf x}-\xi,s|{\bf x}_0) =
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}\left[W({\bf x}+\xi,\tau|{\bf x})W({\bf x},s|{\bf x}_0)
\right].
\]
We can then rewrite the ESKC equation as 
\[
\frac{\partial W({\bf x},s|{\bf x}_0)}{\partial s}\tau=-W({\bf x},s|{\bf x}_0)+
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}
\left[W({\bf x},s|{\bf x}_0)\int_{-\infty}^{\infty} \xi^nW({\bf x}+\xi,\tau|{\bf x})d\xi\right].
\]
We have neglected higher powers of $\tau$ and have used that for $n=0$ 
we get simply $W({\bf x},s|{\bf x}_0)$ due to normalization.
}
\end{small}
  %\end{block}
}




\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equations}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
We simplify the above by introducing the moments 
\[
M_n=\frac{1}{\tau}\int_{-\infty}^{\infty} \xi^nW({\bf x}+\xi,\tau|{\bf x})d\xi=
\frac{\langle [\Delta x(\tau)]^n\rangle}{\tau},
\]
resulting in
\[
\frac{\partial W({\bf x},s|{\bf x}_0)}{\partial s}=
\sum_{n=1}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}
\left[W({\bf x},s|{\bf x}_0)M_n\right].
\]
}
\end{small}
  %\end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equations}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
When $\tau \rightarrow 0$ we assume that $\langle [\Delta x(\tau)]^n\rangle \rightarrow 0$ more rapidly than $\tau$ itself if $n > 2$. 
When $\tau$ is much larger than the standard correlation time of 
system then $M_n$ for $n > 2$ can normally be neglected.
This means that fluctuations become negligible at large time scales.

If we neglect such terms we can rewrite the ESKC equation as 
\[
\frac{\partial W({\bf x},s|{\bf x}_0)}{\partial s}=
-\frac{\partial M_1W({\bf x},s|{\bf x}_0)}{\partial x}+
\frac{1}{2}\frac{\partial^2 M_2W({\bf x},s|{\bf x}_0)}{\partial x^2}.
\]
}
\end{small}
  %\end{block}
}
\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equations}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
In a more compact form we have
\[
\frac{\partial W}{\partial s}=
-\frac{\partial M_1W}{\partial x}+
\frac{1}{2}\frac{\partial^2 M_2W}{\partial x^2},
\]
which is the Fokker-Planck equation!  It is trivial to replace 
position with velocity (momentum).
}
\end{small}
  %\end{block}
}

\frame
{
  \frametitle{Langevin equation}
\begin{small}
{\scriptsize
Consider a particle  suspended in a liquid. On its path through the liquid it will continuously collide with the liquid molecules. Because on average the particle  will collide more often on the front side than on the back side, it will experience a systematic force proportional with its velocity, and directed opposite to its velocity. Besides this systematic force the particle  will experience a stochastic force  $ \vec{F}(t)$. 
The equations of motion then read 
\[ 
 \frac{d\vec{r}}{dt} 	=  \vec{v},
\] 	
\[
\frac{d\vec{v}}{dt} 	=  -\xi \vec{v}+\vec{F}.
\]


}
\end{small}
}
\frame
{
  \frametitle{Langevin equation}
\begin{small}
{\scriptsize
From hydrodynamics  we know that the friction constant  $\xi$ is given by

\begin{displaymath}\xi =6\pi \eta a/m \end{displaymath}

where $\eta$ is the viscosity  of the solvent and a is the radius of the particle .

Solving the second equation in the previous slide we get 
\[
\vec{v}(t)=\vec{v}_{0}e^{-\xi t}+\int_{0}^{t}d\tau e^{-\xi (t-\tau )}\vec{F }(\tau ). 
\]
}
\end{small}
}
\frame
{
  \frametitle{Langevin equation}
\begin{small}
{\scriptsize
If we want to get some useful information out of this, we have to average over all possible realizations of 
$ \vec{F}(t)$, with the initial velocity as a condition. A useful quantity for example is
 \[ 
\langle \vec{v}(t)\cdot \vec{v}(t)\rangle_{\vec{v}_{0}}=v_{0}^{-\xi 2t}
+2\int_{0}^{t}d\tau e^{-\xi (2t-\tau)}\vec{v}_{0}\cdot \langle \vec{F}(\tau )\rangle_{\vec{v}_{0}}
\]
\[  	  	
 +\int_{0}^{t}d\tau ^{\prime }\int_{0}^{t}d\tau e^{-\xi (2t-\tau -\tau ^{\prime })}
\langle \vec{F}(\tau )\cdot \vec{F}(\tau ^{\prime })\rangle_{ \vec{v}_{0}}.
\]
}
\end{small}
}
\frame
{
  \frametitle{Langevin equation}
\begin{small}
{\scriptsize
In order to continue we have to make some assumptions about the conditional averages of the stochastic forces. 
In view of the chaotic character of the stochastic forces the following assumptions seem to be appropriate
  
\[ \langle \vec{F}(t)\rangle 	= 	0, \]
\[\langle \vec{F}(t)\cdot \vec{F}(t^{\prime })\rangle_{\vec{v}_{0}}=  C_{\vec{v}_{0}}\delta (t-t^{\prime }).
\] 	

}
\end{small}
}
\frame
{
  \frametitle{Langevin equation}
\begin{small}
{\scriptsize
We omit the subscript $\vec{v}_{0}$, when the quantity of interest turns out to be independent of $\vec{v}_{0}$. Using the last three equations we get

 \[
\langle \vec{v}(t)\cdot \vec{v}(t)\rangle_{\vec{v}_{0}}=v_{0}^{2}e^{-2\xi t}+\frac{C_{\vec{v}_{0}}}{2\xi }(1-e^{-2\xi t}).\]

For large t this should be equal to 3kT/m, from which it follows that

\[
\langle \vec{F}(t)\cdot \vec{F}(t^{\prime })\rangle =6\frac{kT}{m}\xi \delta (t-t^{\prime }). \]

This result is called the fluctuation-dissipation theorem .
}
\end{small}
}
\frame
{
  \frametitle{Langevin equation}
\begin{small}
{\scriptsize
Integrating 
 \[ 
\vec{v}(t)=\vec{v}_{0}e^{-\xi t}+\int_{0}^{t}d\tau e^{-\xi (t-\tau )}\vec{F }(\tau ), \] 
we get
\[
\vec{r}(t)=\vec{r}_{0}+\vec{v}_{0}\frac{1}{\xi }(1-e^{-\xi t})+
\int_0^td\tau \int_0^{\tau}\tau ^{\prime } e^{-\xi (\tau -\tau ^{\prime })}\vec{F}(\tau ^{\prime }), \]
from which we calculate the mean square displacement 
\[
\langle ( \vec{r}(t)-\vec{r}_{0})^{2}\rangle _{\vec{v}_{0}}=\frac{v_0^2}{\xi}(1-e^{-\xi t})^{2}+\frac{3kT}{m\xi ^{2}}(2\xi t-3+4e^{-\xi t}-e^{-2\xi t}). \]
}
\end{small}
}
\frame
{
  \frametitle{Langevin equation}
\begin{small}
{\scriptsize
For very large $t$ this becomes

\[
\langle (\vec{r}(t)-\vec{r}_{0})^{2}\rangle =\frac{6kT}{m\xi }t \] 

from which we get the Einstein relation  

 \[ D= \frac{kT}{m\xi } \] 	

where we have used $\langle (\vec{r}(t)-\vec{r}_{0})^{2}\rangle =6Dt$.
}
\end{small}
}


\section[Week 8]{Week 8}
\frame
{
  \frametitle{Topics for Week 8, February 18-22}
  \begin{block}{Slater determinants for systems with more than two particles}
\begin{itemize}
\item Repetition from last time
\item Construction of the Slater determinant.
\end{itemize}
Project work this week: try to finalize 1c with importance sampling.  Start implementing functions for Slater determinants.
  \end{block}
} 

\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
The potentially most time-consuming part is the
evaluation of the gradient and the Laplacian of an $N$-particle  Slater
determinant. We have to differentiate the determinant with respect to
all spatial coordinates of all particles. A brute force
differentiation would involve $N\cdot d$ evaluations of the entire
determinant which would even worsen the already undesirable time
scaling, making it $Nd\cdot\bigO(N^3)\sim \bigO(d\cdot N^4)$.
This poses serious hindrances to the overall efficiency of our code.


Before we proceed we need however to define the determinant and other quantities. This involves the computation of the expectation value of the energy. The latter will also serve as a useful test for our algorithms.
}
\end{small}
}


\frame
{
\frametitle{Definitions}
The quantum mechanical wave function of a given state with quantum numbers $\lambda$ (encompassing all quantum numbers needed to specify the system), ignoring time, is with $N$ particles (electrons in our case)
\[
\Psi_{\lambda}=\Psi_{\lambda}(x_1,x_2,\dots,x_N),
\]
with $x_i=({\bf r}_i,\sigma_i)$ and the projection of $\sigma_i$ takes the values
$\{-1/2,+1/2\}$ for fermions with spin $1/2$. 
We will hereafter always refer to $\Psi_{\lambda}$ as the exact wave function, and if the ground state is not degenerate we label it as 
\[
\Psi_0=\Psi_0(x_1,x_2,\dots,x_N).
\]

}


\frame
{
\frametitle{Definitions}
Since the solution $\Psi_{\lambda}$ seldomly can be found in closed form, approximations are sought. In this text we define an approximative wave function or an ansatz to the exact wave function as 
\[
\Phi_{\lambda}=\Phi_{\lambda}(x_1,x_2,\dots,x_N),
\]
with 
\[
\Phi_0=\Phi_0(x_1,x_2,\dots,x_N),
\]
being the ansatz to the ground state.  
}


\frame
{
\frametitle{Definitions}
The wave function $\Psi_{\lambda}$ is sought in the Hilbert space of either symmetric or anti-symmetric $N$-body functions, namely
\[
\Psi_{\lambda}\in {\cal H}_N:= {\cal H}_1\oplus{\cal H}_1\oplus\dots\oplus{\cal H}_1,
\]
where the single-particle Hilbert space ${\cal H}_1$ is the space of square integrable functions over
$\in {\mathbb{R}}^{d}\oplus (\sigma)$
resulting in
\[
{\cal H}_1:= L^2(\mathbb{R}^{d}\oplus (\sigma)).
\]
}



\frame
{
\frametitle{Definitions}
Our Hamiltonian is invariant under the permutation (interchange) of two fermions.
Since we deal with fermions however, the total wave function is antisymmetric.
Let $\hat{P}$ be an operator which interchanges two fermions.
Due to the symmetries we have ascribed to our Hamiltonian, this operator commutes with the total Hamiltonian,
\[
[\hat{H},\hat{P}] = 0,
\]
meaning that $\Psi_{\lambda}(x_1, x_2, \dots , x_N)$ is an eigenfunction of 
$\hat{P}$ as well, that is
\[
\hat{P}_{ij}\Psi_{\lambda}(x_1, x_2, \dots,x_i,\dots,x_j,\dots,x_N)=
\beta\Psi_{\lambda}(x_1, x_2, \dots,x_j,\dots,x_i,\dots,x_N),
\]
where $\beta$ is the eigenvalue of $\hat{P}$. We have introduced the suffix $ij$ in order to indicate that we permute fermions $i$ and $j$.
The Pauli principle tells us that the total wave function for a system of fermions
has to be antisymmetric, resulting in the eigenvalue $\beta = -1$.   

}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
The Schr\"odinger equation reads 
\begin{equation}
\hat{H}(x_1, x_2, \dots , x_N) \Psi_{\lambda}(x_1, x_2, \dots , x_N) = 
E_\lambda  \Psi_\lambda(x_1, x_2, \dots , x_N), 
\label{eq:basicSE1}
\end{equation}
where the vector $x_i$ represents the coordinates (spatial and spin) of particle $i$, $\lambda$ stands  for all the quantum
numbers needed to classify a given $N$-particle  state and $\Psi_{\lambda}$ is the pertaining eigenfunction.  Throughout this course,
$\Psi$ refers to the exact eigenfunction, unless otherwise stated.
}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
We write the Hamilton operator, or Hamiltonian,  in a generic way 
\[
	\hat{H} = \hat{T} + \hat{V} 
\]
where $\hat{T}$  represents the kinetic energy of the system
\[
	\hat{T} = \sum_{i=1}^N \frac{\mathbf{p}_i^2}{2m_i} = \sum_{i=1}^N \left( -\frac{\hbar^2}{2m_i} \mathbf{\nabla_i}^2 \right) =
		\sum_{i=1}^N t(x_i)
\]
while the operator $\hat{V}$ for the potential energy is given by
\begin{equation}
	\hat{V} = \sum_{i=1}^N \hat{u}_{\mathrm{ext}}(x_i) + \sum_{ji=1}^N v(x_i,x_j)+\sum_{ijk=1}^Nv(x_i,x_j,x_k)+\dots
\label{eq:firstv}
\end{equation}
Hereafter we use natural units, viz.~$\hbar=c=e=1$, with $e$ the elementary charge and $c$ the speed of light. This means that momenta and masses
have dimension energy. 
}
\end{small}
}
\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
If one does quantum chemistry or atomic physics as we do here, after having introduced the  Born-Oppenheimer approximation which effectively freezes out the nucleonic degrees
of freedom, the Hamiltonian for $N=n_e$ electrons takes the following form 
\[
  \hat{H} = \sum_{i=1}^{n_e} t(x_i) 
  - \sum_{i=1}^{n_e} k\frac{Z}{r_i} + \sum_{i<j}^{n_e} \frac{k}{r_{ij}},
\]
with $k=1.44$ eVnm
}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
 We can rewrite this as
\begin{equation}
    \hat{H} = \hat{H_0} + \hat{H_I} 
    = \sum_{i=1}^{n_e}\hat{h}_0(x_i) + \sum_{i<j=1}^{n_e}\frac{1}{r_{ij}},
\label{H1H2}
\end{equation}
where  we have defined $r_{ij}=| {\bf r}_i-{\bf r}_j|$ and
\begin{equation}
  \hat{h}_0(x_i) =  \hat{t}(x_i) - \frac{Z}{x_i}.
\label{hi}
\end{equation}
The first term of eq.~(\ref{H1H2}), $H_0$, is the sum of the $N$
\emph{one-body} Hamiltonians $\hat{h}_0$. Each individual
Hamiltonian $\hat{h}_0$ contains the kinetic energy operator of an
electron and its potential energy due to the attraction of the
nucleus. The second term, $H_I$, is the sum of the $n_e(n_e-1)/2$
two-body interactions between each pair of electrons. Note that the double sum carries a restriction $i<j$.
}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
The potential energy term due to the attraction of the nucleus defines the one-body field $u_i=u_{\mathrm{ext}}(x_i)$ of Eq.~(\ref{eq:firstv}).
We have moved this term into the $\hat{H}_0$ part of the Hamiltonian, instead of keeping  it in $\hat{V}$ as in  Eq.~(\ref{eq:firstv}).
The reason is that we will hereafter treat $\hat{H}_0$ as our non-interacting  Hamiltonian. For a many-body wavefunction $\Phi_{\lambda}$ defined by an  
appropriate single-particle basis, we may solve exactly the non-interacting eigenvalue problem 
\[
\hat{H}_0\Phi_{\lambda}= w_{\lambda}\Phi_{\lambda},
\]
with $w_{\lambda}$ being the non-interacting energy. This energy is defined by the sum over single-particle energies to be defined below.
For atoms the single-particle energies could be the hydrogen-like single-particle energies corrected for the charge $Z$. For nuclei and quantum
dots, these energies could be given by the harmonic oscillator in three and two dimensions, respectively.
}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
We will assume that the interacting part of the Hamiltonian
can be approximated by a two-body interaction.
This means that our Hamiltonian is written as 
\begin{equation}
    \hat{H} = \hat{H_0} + \hat{H_I} 
    = \sum_{i=1}^N \hat{h}_0(x_i) + \sum_{i<j=1}^N \hat{v}(x_{ij}),
\label{Hnuclei}
\end{equation}
with 
\begin{equation}
  H_0=\sum_{i=1}^N \hat{h}_0(x_i) =  \sum_{i=1}^N\left(\hat{t}(x_i) + \hat{u}_{\mathrm{ext}}(x_i)\right).
\label{hinuclei}
\end{equation}
The one-body part $u_{\mathrm{ext}}(x_i)$ is normally approximated by a harmonic oscillator potential or the Coulomb interaction an electron feels from the nucleus. However, other potentials are fully possible, such as 
one derived from the self-consistent solution of the Hartree-Fock equations or so-called Woods-Saxon potentials used in nuclear physics.
}
\end{small}
}



\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
Our Hamiltonian is invariant under the permutation (interchange) of two fermions. % (exercise here, prove it)
Since we deal with fermions, the total wave function is antisymmetric.
Let $\hat{P}$ be an operator which interchanges two fermions.
Due to the symmetries we have ascribed to our Hamiltonian, this operator commutes with the total Hamiltonian,
\[
[\hat{H},\hat{P}] = 0,
\]
meaning that $\Psi_{\lambda}(x_1, x_2, \dots , x_N)$ is an eigenfunction of 
$\hat{P}$ as well, that is
\[
\hat{P}_{ij}\Psi_{\lambda}(x_1, x_2, \dots,x_i,\dots,x_j,\dots,x_N)=
\beta\Psi_{\lambda}(x_1, x_2, \dots,x_i,\dots,x_j,\dots,x_N),
\]
where $\beta$ is the eigenvalue of $\hat{P}$. We have introduced the suffix $ij$ in order to indicate that we permute fermions $i$ and $j$.
The Pauli principle tells us that the total wave function for a system of fermions
has to be antisymmetric, resulting in the eigenvalue $\beta = -1$.   
}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
In our case we assume that  we can approximate the exact eigenfunction with a Slater determinant
\be
   \Phi(x_1, x_2,\dots ,x_N,\alpha,\beta,\dots, \sigma)=\frac{1}{\sqrt{N!}}
\left| \begin{array}{ccccc} \psi_{\alpha}(x_1)& \psi_{\alpha}(x_2)& \dots & \dots & \psi_{\alpha}(x_N)\\
                            \psi_{\beta}(x_1)&\psi_{\beta}(x_2)& \dots & \dots & \psi_{\beta}(x_N)\\  
                            \dots & \dots & \dots & \dots & \dots \\
                            \dots & \dots & \dots & \dots & \dots \\
                     \psi_{\sigma}(x_1)&\psi_{\sigma}(x_2)& \dots & \dots & \psi_{\sigma}(x_N)\end{array} \right|, 
\label{HartreeFockDet}
\ee 
where  $x_i$  stand for the coordinates and spin values of a particle $i$ and $\alpha,\beta,\dots, \gamma$ 
are quantum numbers needed to describe remaining quantum numbers.  
}
\end{small}
}


\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
Let us denote the ground state energy by $E_0$. According to the
variational principle we have
\begin{equation*}
  E_0 \le E[\Phi] = \int \Phi^*\hat{H}\Phi d\mathbf{\tau}
\end{equation*}
where $\Phi$ is a trial function which we assume to be normalized
\begin{equation*}
  \int \Phi^*\Phi d\mathbf{\tau} = 1,
\end{equation*}
where we have used the shorthand $d\mathbf{\tau}=d\mathbf{x}_1d\mathbf{x}_2\dots d\mathbf{x}_N$.
}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
In the Hartree-Fock method the trial function is the Slater
determinant of Eq.~(\ref{HartreeFockDet}) which can be rewritten as 
\begin{equation}
  \Phi(x_1,x_2,\dots,x_N,\alpha,\beta,\dots,\nu) = \frac{1}{\sqrt{N!}}\sum_{P} (-)^P\hat{P}\psi_{\alpha}(x_1)
    \psi_{\beta}(x_2)\dots\psi_{\nu}(x_N)=\sqrt{N!}{\cal A}\Phi_H,
\label{HartreeFockPermutation}
\end{equation}
where we have introduced the antisymmetrization operator ${\cal A}$ defined by the 
summation over all possible permutations of two fermions.
}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
It is defined as
\begin{equation}
  {\cal A} = \frac{1}{N!}\sum_{p} (-)^p\hat{P},
\label{antiSymmetryOperator}
\end{equation}
with $p$ standing for the number of permutations. We have introduced for later use the so-called
Hartree-function, defined by the simple product of all possible single-particle functions
\begin{equation*}
  \Phi_H(x_1,x_2,\dots,x_N,\alpha,\beta,\dots,\nu) =
  \psi_{\alpha}(x_1)
    \psi_{\beta}(x_2)\dots\psi_{\nu}(x_N).
\end{equation*}

}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
Both $\hat{H_0}$ and $\hat{\hat{H}_I}$ are invariant under all possible permutations of any two fermions
and hence commute with ${\cal A}$
\begin{equation}
  [H_0,{\cal A}] = [H_I,{\cal A}] = 0.
  \label{cummutionAntiSym}
\end{equation}
Furthermore, ${\cal A}$ satisfies
\begin{equation}
  {\cal A}^2 = {\cal A},
  \label{AntiSymSquared}
\end{equation}
since every permutation of the Slater
determinant reproduces it. 
}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
The expectation value of $\hat{H_0}$ 
\[
  \int \Phi^*\hat{H_0}\Phi d\mathbf{\tau} 
  = N! \int \Phi_H^*{\cal A}\hat{H_0}{\cal A}\Phi_H d\mathbf{\tau}
\]
is readily reduced to
\[
  \int \Phi^*\hat{H_0}\Phi d\mathbf{\tau} 
  = N! \int \Phi_H^*\hat{H_0}{\cal A}\Phi_H d\mathbf{\tau},
\]
where we have used eqs.~(\ref{cummutionAntiSym}) and
(\ref{AntiSymSquared}). The next step is to replace the antisymmetrization
operator by its definition Eq.~(\ref{HartreeFockPermutation}) and to
replace $\hat{H_0}$ with the sum of one-body operators
\[
  \int \Phi^*\hat{H_0}\Phi  d\mathbf{\tau}
  = \sum_{i=1}^N \sum_{p} (-)^p\int 
  \Phi_H^*\hat{h}_0\hat{P}\Phi_H d\mathbf{\tau}.
\]

}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
The integral vanishes if two or more fermions are permuted in only one
of the Hartree-functions $\Phi_H$ because the individual single-particle wave functions are
orthogonal. We obtain then
\[
  \int \Phi^*\hat{H}_0\Phi  d\mathbf{\tau}= \sum_{i=1}^N \int \Phi_H^*\hat{h}_0\Phi_H  d\mathbf{\tau}.
\]
Orthogonality of the single-particle functions allows us to further simplify the integral, and we
arrive at the following expression for the expectation values of the
sum of one-body Hamiltonians 
\begin{equation}
  \int \Phi^*\hat{H}_0\Phi  d\mathbf{\tau}
  = \sum_{\mu=1}^N \int \psi_{\mu}^*(\mathbf{x})\hat{h}_0\psi_{\mu}(\mathbf{x})
  d\mathbf{x}.
  \label{H1Expectation}
\end{equation}

}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
We introduce the following shorthand for the above integral
\[
\langle \mu | \hat{h}_0 | \mu \rangle = \int \psi_{\mu}^*(\mathbf{x})\hat{h}_0\psi_{\mu}(\mathbf{x})d\mathbf{x}.,
\]
and rewrite Eq.~(\ref{H1Expectation}) as
\begin{equation}
  \int \Phi^*\hat{H_0}\Phi  d\mathbf{\tau}
  = \sum_{\mu=1}^N \langle \mu | \hat{h}_0 | \mu \rangle.
  \label{H1Expectation1}
\end{equation}

}
\end{small}
}
\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
The expectation value of the two-body part of the Hamiltonian (assuming a two-body Hamiltonian at most) is obtained in a
similar manner. We have
\begin{equation*}
  \int \Phi^*\hat{H_I}\Phi d\mathbf{\tau} 
  = N! \int \Phi_H^*{\cal A}\hat{H_I}{\cal A}\Phi_H d\mathbf{\tau},
\end{equation*}
which reduces to
\begin{equation*}
 \int \Phi^*\hat{H_I}\Phi d\mathbf{\tau} 
  = \sum_{i\le j=1}^N \sum_{p} (-)^p\int 
  \Phi_H^*\hat{v}(x_{ij})\hat{P}\Phi_H d\mathbf{\tau},
\end{equation*}
by following the same arguments as for the one-body
Hamiltonian. 
}
\end{small}
}
\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
Because of the dependence on the inter-particle distance $r_{ij}$,  permutations of
any two fermions no longer vanish, and we get
\begin{equation*}
  \int \Phi^*\hat{H_I}\Phi d\mathbf{\tau} 
  = \sum_{i < j=1}^N \int  
  \Phi_H^*\hat{v}(x_{ij})(1-P_{ij})\Phi_H d\mathbf{\tau}.
\end{equation*}
where $P_{ij}$ is the permutation operator that interchanges
particle $i$ and particle $j$. Again we use the assumption that the single-particle wave functions
are orthogonal. 
}
\end{small}
}
\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
We obtain
\begin{equation}
\begin{split}
  \int \Phi^*\hat{H_I}\Phi d\mathbf{\tau} 
  = \frac{1}{2}\sum_{\mu=1}^N\sum_{\nu=1}^N
    &\left[ \int \psi_{\mu}^*(x_i)\psi_{\nu}^*(x_j)\hat{v}(x_{ij})\psi_{\mu}(x_i)\psi_{\nu}(x_j)
    dx_idx_j \right.\\
  &\left.
  - \int \psi_{\mu}^*(x_i)\psi_{\nu}^*(x_j)
  \hat{v}(x_{ij})\psi_{\nu}(x_i)\psi_{\mu}(x_j)
  dx_idx_j
  \right]. \label{H2Expectation}
\end{split}
\end{equation}
The first term is the so-called direct term. In Hartree-Fock theory it leads to the so-called Hartree term, 
while the second is due to the Pauli principle and is called
the exchange term and in Hartree-Fock theory it defines the so-called Fock term.
The factor  $1/2$ is introduced because we now run over
all pairs twice. 
}
\end{small}
}
\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
The last equation allows us to  introduce some further definitions.  
The single-particle wave functions $\psi_{\mu}({\bf x})$, defined by the quantum numbers $\mu$ and ${\bf x}$
(recall that ${\bf x}$ also includes spin degree, later we will also add isospin)   are defined as the overlap 
\[
   \psi_{\alpha}(x)  = \langle x | \alpha \rangle .
\]

}
\end{small}
}
\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
We introduce the following shorthands for the above two integrals
\[
\langle \mu\nu|V|\mu\nu\rangle =  \int \psi_{\mu}^*(x_i)\psi_{\nu}^*(x_j)\hat{v}(x_{ij})\psi_{\mu}(x_i)\psi_{\nu}(x_j)
    dx_idx_j,
\]
and 
\[
\langle \mu\nu|V|\nu\mu\rangle = \int \psi_{\mu}^*(x_i)\psi_{\nu}^*(x_j)
  \hat{v}(x_{ij})\psi_{\nu}(x_i)\psi_{\mu}(x_j)
  dx_idx_j.  
\]
}
\end{small}
}
\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
The direct and exchange matrix elements can be  brought together if we define the antisymmetrized matrix element
\[
\langle \mu\nu|V|\mu\nu\rangle_{\mathrm{AS}}= \langle \mu\nu|V|\mu\nu\rangle-\langle \mu\nu|V|\nu\mu\rangle,
\]
or for a general matrix element  
\[
\langle \mu\nu|V|\sigma\tau\rangle_{\mathrm{AS}}= \langle \mu\nu|V|\sigma\tau\rangle-\langle \mu\nu|V|\tau\sigma\rangle.
\]
It has the symmetry property
\[
\langle \mu\nu|V|\sigma\tau\rangle_{\mathrm{AS}}= -\langle \mu\nu|V|\tau\sigma\rangle_{\mathrm{AS}}=-\langle \nu\mu|V|\sigma\tau\rangle_{\mathrm{AS}}.
\]
}
\end{small}
}
\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
The antisymmetric matrix element is also hermitian, implying 
\[
\langle \mu\nu|V|\sigma\tau\rangle_{\mathrm{AS}}= \langle \sigma\tau|V|\mu\nu\rangle_{\mathrm{AS}}.
\]

With these notations we rewrite Eq.~(\ref{H2Expectation}) as 
\begin{equation}
  \int \Phi^*\hat{H_I}\Phi d\mathbf{\tau} 
  = \frac{1}{2}\sum_{\mu=1}^N\sum_{\nu=1}^N \langle \mu\nu|V|\mu\nu\rangle_{\mathrm{AS}}.
\label{H2Expectation2}
\end{equation}

}
\end{small}
}
\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
Combining Eqs.~(\ref{H1Expectation1}) and
(\ref{H2Expectation2}) we obtain the energy functional 
\begin{equation}
  E[\Phi] 
  = \sum_{\mu=1}^N \langle \mu | \hat{h}_0 | \mu \rangle +
  \frac{1}{2}\sum_{{\mu}=1}^N\sum_{{\nu}=1}^N \langle \mu\nu|V|\mu\nu\rangle_{\mathrm{AS}}.
\label{FunctionalEPhi}
\end{equation}
This equation is very useful, in particular if we only look at the unperturbed part $H_0$. This part can be represented by closed form expressions that can be used to check our algorithms.
}
\end{small}
}



\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize

We are now ready to start implementing the coding of the Slater determinant.
The potentially most time-consuming part is the
evaluation of the gradient and the Laplacian of an $N$-particle  Slater
determinant. We have to differentiate the determinant with respect to
all spatial coordinates of all particles. A brute force
differentiation would involve $N\cdot d$ evaluations of the entire
determinant which would even worsen the already undesirable time
scaling, making it $Nd\cdot\bigO(N^3)\sim \bigO(d\cdot N^4)$.
This poses serious hindrances to the overall efficiency of our code.

The efficiency can be improved however if we move only one electron at the time.
The Slater determinant matrix $\matr D$ is defined by the matrix elements
\be
d_{ij}\equiv\phi_j(x_i)
\ee
where $\phi_j(\mathbf{r}_i)$ is a single particle  wave function.
The columns correspond to the position of a given particle 
while the rows stand for the various quantum numbers.
}
\end{small}
}

\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
What we need to realize is that when differentiating a Slater
determinant with respect to some given coordinate, only one row of the
corresponding Slater matrix is changed. Therefore, by recalculating
the whole determinant we risk producing redundant information. The
solution turns out to be an algorithm that requires to keep track of
the \emph{inverse} of the Slater matrix.

Let the
current position in phase space be represented by the $(N\cdot
d)$-element vector $\mathbf{r}^{\mathrm{old}}$ and the new suggested
position by the vector $\mathbf{r}^{\mathrm{new}}$.


The inverse of $\matr D$ can be expressed in terms of its
cofactors $C_{ij}$ and its determinant $\det{\matr D}$:
\be
d_{ij}^{-1} = \frac{C_{ji}}{\det{\matr D}}
\label{eq:inverse_cofactor}
\ee
Notice that the interchanged indices indicate that the matrix of
cofactors is to be transposed.
}
\end{small}
}

\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
If $\matr D$ is invertible, then we must obviously have $\matr
D^{-1}\matr D= {\bf 1}$, or explicitly in terms of the individual
elements of $\matr D$ and $\matr D^{-1}$:
\be
\sum_{k=1}^N d_{ik}^{\phantom X}d_{kj}^{-1} = \delta_{ij}^{\phantom X}
\label{eq:unity_explicitely}
\ee
Consider the ratio, which we shall call $R$, between $\det{\matr
  D(\mathbf{r}^{\mathrm{new}})}$ and $\det{\matr D(\mathbf{r}^{\mathrm{old}})}$. 
By definition, each of these determinants can
individually be expressed in terms of the $i$th row of its cofactor
matrix
\be
R\equiv\frac{\det{\matr D(\mathbf{r}^{\mathrm{new}})}}
{\det{\matr D(\mathbf{r}^{\mathrm{old}})}} =
\frac{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{new}})\,
C_{ij}(\mathbf{r}^{\mathrm{new}})}
{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{old}})\,
C_{ij}(\mathbf{r}^{\mathrm{old}})}
\label{eq:detratio_cofactors}
\ee
}
\end{small}
}

\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
Suppose now that we move only one particle  at a time, meaning that
$\mathbf{r}^{\mathrm{new}}$ differs from $\mathbf{r}^{\mathrm{old}}$ by the
position of only one, say the $i$th, particle . This means that $\matr
D(\mathbf{r}^{\mathrm{new}})$ and $\matr D(\mathbf{r}^{\mathrm{old}})$ differ
only by the entries of the $i$th row.  Recall also that the $i$th row
of a cofactor matrix $\matr C$ is independent of the entries of the
$i$th row of its corresponding matrix $\matr D$. In this particular
case we therefore get that the $i$th row of $\matr C(\mathbf{r}^{\mathrm{new}})$ 
and $\matr C(\mathbf{r}^{\mathrm{old}})$ must be
equal. Explicitly, we have:
\be
C_{ij}(\mathbf{r}^{\mathrm{new}}) = C_{ij}(\mathbf{r}^{\mathrm{old}})\quad
\forall\ j\in\{1,\dots,N\}
\ee
}
\end{small}
}

\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
Inserting this into the numerator of eq.~(\ref{eq:detratio_cofactors})
and using eq.~(\ref{eq:inverse_cofactor}) to substitute the cofactors
with the elements of the inverse matrix, we get:
\be
%\frac{\det{\matr D(\mathbf{r}^{\mathrm{new}})}}
%{\det{\matr D(\mathbf{r}^{\mathrm{old}})}}
R =
\frac{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{new}})\,
C_{ij}(\mathbf{r}^{\mathrm{old}})}
{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{old}})\,
C_{ij}(\mathbf{r}^{\mathrm{old}})} =
\frac{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{new}})\,
d_{ji}^{-1}(\mathbf{r}^{\mathrm{old}})}
{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{old}})\,
d_{ji}^{-1}(\mathbf{r}^{\mathrm{old}})}
\ee
}
\end{small}
}

\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
Now by eq.~(\ref{eq:unity_explicitely}) the denominator of the rightmost
expression must be unity, so that we finally arrive at:
\be
R =
%\frac{\det{\matr D(\mathbf{r}^{\mathrm{new}})}}
%{\det{\matr D(\mathbf{r}^{\mathrm{old}})}} =
\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{new}})\,
d_{ji}^{-1}(\mathbf{r}^{\mathrm{old}}) = 
\sum_{j=1}^N \phi_j(\mathbf{r}_i^{\mathrm{new}})\,
d_{ji}^{-1}(\mathbf{r}^{\mathrm{old}})
\label{eq:detratio_inverse}
\ee
What this means is that in order to get the ratio when only the $i$th
particle  has been moved, we only need to calculate the dot
product of the vector $\left(\phi_1(\mathbf{r}_i^\mathrm{new}),\,\dots,\,
\phi_N(\mathbf{r}_i^\mathrm{new})\right)$ of single particle  wave functions
evaluated at this new position with the $i$th column of the inverse
matrix $\matr D^{-1}$ evaluated at the original position. Such
an operation has a time scaling of $\bigO(N)$. The only extra thing we
need to do is to maintain the inverse matrix $\matr D^{-1}(\vec
x^{\mathrm{old}})$.
}
\end{small}
}

\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
If the new position $\mathbf{r}^{\mathrm{new}}$ is accepted, then the
inverse matrix can by suitably updated by an algorithm having a time
scaling of $\bigO(N^2)$.  This algorithm goes as
follows. First we update all but the $i$th column of $\matr
D^{-1}$. For each column $j\neq i$, we first calculate the quantity:
\be
S_j =
(\matr D(\mathbf{r}^{\mathrm{new}})\times
\matr D^{-1}(\mathbf{r}^{\mathrm{old}}))_{ij} =
\sum_{l=1}^N d_{il}(\mathbf{r}^{\mathrm{new}})\,
d^{-1}_{lj}(\mathbf{r}^{\mathrm{old}})
\label{eq:inverse_update_1}
\ee
The new elements of the $j$th column of $\matr D^{-1}$ are then given
by:
\be
d_{kj}^{-1}(\mathbf{r}^{\mathrm{new}}) =
d_{kj}^{-1}(\mathbf{r}^{\mathrm{old}}) -
\frac{S_j}{R}\,d_{ki}^{-1}(\mathbf{r}^{\mathrm{old}})\quad
\begin{array}{ll}
\forall\ \ k\in\{1,\dots,N\}\\j\neq i
\end{array}
\label{eq:inverse_update_2}
\ee
}
\end{small}
}

\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
Finally the elements of the $i$th column of $\matr D^{-1}$ are updated
simply as follows:
\be
d_{ki}^{-1}(\mathbf{r}^{\mathrm{new}}) =
\frac{1}{R}\,d_{ki}^{-1}(\mathbf{r}^{\mathrm{old}})\quad
\forall\ \ k\in\{1,\dots,N\}
\label{eq:inverse_update_3}
\ee
We see from these formulas that the time scaling of an update of
$\matr D^{-1}$ after changing one row of $\matr D$ is $\bigO(N^2)$.
}
\end{small}
}


\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
The scheme is also applicable for the calculation of the ratios
involving derivatives. It turns
out that differentiating the Slater determinant with respect
to the coordinates of a single particle  $\mathbf{r}_i$ changes only the
$i$th row of the corresponding Slater matrix. 
}
\end{small}
}




\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
The gradient and
Laplacian can therefore be calculated as follows:
\be
\frac{\vec\nabla_i\det{\matr D(\mathbf{r})}}
{\det{\matr D(\mathbf{r})}} =
\sum_{j=1}^N \vec\nabla_i d_{ij}(\mathbf{r})\,
d_{ji}^{-1}(\mathbf{r}) =
\sum_{j=1}^N \vec\nabla_i \phi_j(\mathbf{r}_i)\,
d_{ji}^{-1}(\mathbf{r})
\ee
and
\be
\frac{\nabla^2_i\det{\matr D(\mathbf{r})}}
{\det{\matr D(\mathbf{r})}} =
\sum_{j=1}^N \nabla^2_i d_{ij}(\mathbf{r})\,
d_{ji}^{-1}(\mathbf{r}) =
\sum_{j=1}^N \nabla^2_i \phi_j(\mathbf{r}_i)\,
d_{ji}^{-1}(\mathbf{r})
\ee
}
\end{small}
}

\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
Thus, to calculate all the derivatives of the Slater determinant, we
only need the derivatives of the single particle  wave functions
($\vec\nabla_i \phi_j(\mathbf{r}_i)$ and $\nabla^2_i \phi_j(\mathbf{r}_i)$)
and the elements of the corresponding inverse Slater matrix ($\matr
D^{-1}(\mathbf{r}_i)$). A calculation of a single derivative is by the
above result an $\bigO(N)$ operation. Since there are $d\cdot N$
derivatives, the time scaling of the total evaluation becomes
$\bigO(d\cdot N^2)$. With an $\bigO(N^2)$ updating algorithm for the
inverse matrix, the total scaling is no worse, which is far better
than the brute force approach yielding $\bigO(d\cdot N^4)$.\newline
{\bf Important note:} In most cases you end with closed form expressions for the single-particle  wave functions. It is then useful to calculate the various derivatives and make separate functions
for them.

}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Slater determinant: Explicit expressions for various Atoms, beryllium}
\begin{small}
{\scriptsize
The Slater determinant takes the form  
\[
   \Phi({\bf r}_1,{\bf r}_2,,{\bf r}_3,{\bf r}_4, \alpha,\beta,\gamma,\delta)=\frac{1}{\sqrt{4!}}
\left| \begin{array}{cccc} \psi_{100\uparrow}({\bf r}_1)& \psi_{100\uparrow}({\bf r}_2)& \psi_{100\uparrow}({\bf r}_3)&\psi_{100\uparrow}({\bf r}_4) \\
\psi_{100\downarrow}({\bf r}_1)& \psi_{100\downarrow}({\bf r}_2)& \psi_{100\downarrow}({\bf r}_3)&\psi_{100\downarrow}({\bf r}_4) \\
\psi_{200\uparrow}({\bf r}_1)& \psi_{200\uparrow}({\bf r}_2)& \psi_{200\uparrow}({\bf r}_3)&\psi_{200\uparrow}({\bf r}_4) \\
\psi_{200\downarrow}({\bf r}_1)& \psi_{200\downarrow}({\bf r}_2)& \psi_{200\downarrow}({\bf r}_3)&\psi_{200\downarrow}({\bf r}_4) \end{array} \right|.
\]

The Slater determinant as written is zero since the spatial wave functions for the spin up and spin down 
states are equal.  
But we can rewrite it as the product of two Slater determinants, one for spin up and one for spin down.
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Slater determinant: Explicit expressions for various Atoms, beryllium}
\begin{small}
{\scriptsize
We can rewrite it as 
\[
   \Phi({\bf r}_1,{\bf r}_2,,{\bf r}_3,{\bf r}_4, \alpha,\beta,\gamma,\delta)=Det\uparrow(1,2)Det\downarrow(3,4)-
Det\uparrow(1,3)Det\downarrow(2,4)
\]
\[
-Det\uparrow(1,4)Det\downarrow(3,2)+Det\uparrow(2,3)Det\downarrow(1,4)-Det\uparrow(2,4)Det\downarrow(1,3)
\]
\[
+Det\uparrow(3,4)Det\downarrow(1,2),
\]
where we have defined
\[
Det\uparrow(1,2)=\frac{1}{\sqrt{2}}\left| \begin{array}{cc} \psi_{100\uparrow}({\bf r}_1)& \psi_{100\uparrow}({\bf r}_2)\\
\psi_{200\uparrow}({\bf r}_1)& \psi_{200\uparrow}({\bf r}_2) \end{array} \right|,
\]
and 
\[
Det\downarrow(3,4)=\frac{1}{\sqrt{2}}\left| \begin{array}{cc} \psi_{100\downarrow}({\bf r}_3)& \psi_{100\downarrow}({\bf r}_4)\\
\psi_{200\downarrow}({\bf r}_3)& \psi_{200\downarrow}({\bf r}_4) \end{array} \right|.
\]

The total determinant is still zero!
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Slater determinant: Explicit expressions for various Atoms, beryllium}
\begin{small}
{\scriptsize

We want to avoid to sum over spin variables, in particular when the interaction does not depend on spin.

It can be shown, see for example Moskowitz and Kalos, Int.~J.~Quantum Chem.~{\bf 20} (1981) 1107, that for the variational energy
we can approximate the Slater determinant as  
\[
   \Phi({\bf r}_1,{\bf r}_2,,{\bf r}_3,{\bf r}_4, \alpha,\beta,\gamma,\delta) \propto Det\uparrow(1,2)Det\downarrow(3,4),
\]
or more generally as 
\[
   \Phi({\bf r}_1,{\bf r}_2,\dots {\bf r}_N) \propto Det\uparrow Det\downarrow,
\]
where we have the Slater determinant as the product of a spin up part involving the number of electrons with spin up only (2 for beryllium and 5 for neon) and a spin down part involving the electrons with spin down.

This ansatz is not antisymmetric under the exchange of electrons with  opposite spins but it can be shown that it gives the same
expectation value for the energy as the full Slater determinant.

As long as the Hamiltonian is spin independent, the above is correct. It is rather straightforward to see this if you go back to the equations for the energy discussed earlier today.
 }
 \end{small}
 }



\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
We will thus
factorize the full determinant $\det{\matr D}$ into two smaller ones, where 
each can be identified with $\uparrow$ and $\downarrow$
respectively:
\be
\det{\matr D} = \det{\matr D}_\uparrow\cdot\det{\matr D}_\downarrow
\ee
The combined dimensionality of the two smaller determinants equals the
dimensionality of the full determinant. Such a factorization is
advantageous in that it makes it possible to perform the calculation
of the ratio $R$ and the updating of the inverse matrix separately for
$\det{\matr D}_\uparrow$ and $\det{\matr D}_\downarrow$:
\be
\frac{\det{\matr D}^\mathrm{new}}{\det{\matr D}^\mathrm{old}} =
\frac{\det{\matr D}^\mathrm{new}_\uparrow}
{\det{\matr D}^\mathrm{old}_\uparrow}\cdot
\frac{\det{\matr D}^\mathrm{new}_\downarrow
}{\det{\matr D}^\mathrm{old}_\downarrow}
\ee
}
\end{small}
}

\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
This reduces the calculation time by a constant factor. The maximal
time reduction happens in a system of equal numbers of $\uparrow$ and
$\downarrow$ particles, so that the two factorized determinants are
half the size of the original one.


Consider the case of moving only one particle  at a time which
originally had the following time scaling for one transition:
\be
\bigO_R(N)+\bigO_\mathrm{inverse}(N^2)
\ee
For the factorized determinants one of the two determinants is
obviously unaffected by the change so that it cancels from the ratio
$R$. 
}
\end{small}
}

\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
Therefore, only one determinant of size $N/2$ is involved in each
calculation of $R$ and update of the inverse matrix. The scaling of
each transition then becomes:
\be
\bigO_R(N/2)+\bigO_\mathrm{inverse}(N^2/4)
\ee
and the time scaling when the transitions for all $N$ particles are
put together:
\be
\bigO_R(N^2/2)+\bigO_\mathrm{inverse}(N^3/4)
\ee
which gives the same reduction as in the case of moving all particles
at once.
}
\end{small}
}


\frame
{
  \frametitle{Updating the Slater matrix}
\begin{small}
{\scriptsize
Computing the ratios discussed above requires that we maintain 
the inverse of the Slater matrix evaluated at the current position. 
Each time a trial position is accepted, the row number $i$ of the Slater 
matrix changes and updating its inverse has to be carried out. 
Getting the inverse of an $N \times N$ matrix by Gaussian elimination has a 
complexity of order of $\mathcal{O}(N^3)$ operations, a luxury that we 
cannot afford for each time a particle  move is accepted.
We will use the expression
\begin{eqnarray}\label{updatingInverse}
\boxed{d^{-1}_{kj}(\bfv{x^{new}})  = \left\{ 
\begin{array}{l l}
  d^{-1}_{kj}(\bfv{x^{old}}) - \frac{d^{-1}_{ki}(\bfv{x^{old}})}{R} \sum_{l=1}^{N} d_{il}(\bfv{x^{new}})  d^{-1}_{lj}(\bfv{x^{old}}) & \mbox{if $j \neq i$}\nonumber \\ \\
 \frac{d^{-1}_{ki}(\bfv{x^{old}})}{R} \sum_{l=1}^{N} d_{il}(\bfv{x^{old}}) d^{-1}_{lj}(\bfv{x^{old}}) & \mbox{if $j=i$}
\end{array} \right.}\\
\end{eqnarray}
}
\end{small}
}


\frame
{
  \frametitle{Updating the Slater matrix}
\begin{small}
{\scriptsize
This equation scales as $O(N^2)$.
The evaluation of the determinant of an $N \times N$ matrix by standard Gaussian elimination requires ${\cal O}(N^3)$
calculations. 
As there are $Nd$ independent coordinates we need to evaluate $Nd$ Slater determinants 
for the gradient (quantum force) and $Nd$ for the Laplacian (kinetic energy). 
With the updating algorithm we need only to invert the Slater 
determinant matrix once. This can be done by standard LU decomposition methods.\\

}
\end{small}
}





\frame
{
  \frametitle{Slater Determinant and  VMC}
\begin{small}
{\scriptsize

Determining a determinant of an $N \times N$ matrix by
standard Gaussian elimination is of the order of ${\cal O}(N^3)$
calculations. As there are $N\cdot d$ independent coordinates we need
to evaluate $Nd$ Slater determinants for the gradient (quantum force) and
$N\cdot d$ for the Laplacian (kinetic energy)

With the updating algorithm we need only to invert the Slater determinant matrix once.
This is done by calling standard LU decomposition methods.
}
\end{small}
}





\frame
{
  \frametitle{How to compute the Slater Determinant}
\begin{small}
{\scriptsize
If you choose to implement the above recipe for the computation of the Slater determinant,
you need to LU decompose the Slater matrix. This is described in chapter 6 of the lecture notes from 2012.

You need to call the function ludcmp in lib.cpp.
You need to transfer the Slater matrix and its dimension. You get back an LU decomposed matrix.
}
\end{small}
}

\frame
{
  \frametitle{LU Decomposition}
\begin{small}
{\scriptsize
The LU decomposition method means that we can rewrite
this matrix as the product of two matrices ${\bf B}$ and ${\bf C}$
where 
\[
\label{eq3}
   \left(\begin{array}{cccc}
                          a_{11} & a_{12} & a_{13} & a_{14} \\
                          a_{21} & a_{22} & a_{23} & a_{24} \\
                          a_{31} & a_{32} & a_{33} & a_{34} \\
                          a_{41} & a_{42} & a_{43} & a_{44} 
                      \end{array} \right)
                      = \left( \begin{array}{cccc}
                              1  & 0      & 0      & 0 \\
                          b_{21} & 1      & 0      & 0 \\
                          b_{31} & b_{32} & 1      & 0 \\
                          b_{41} & b_{42} & b_{43} & 1 
                      \end{array} \right) 
                        \left( \begin{array}{cccc}
                          c_{11} & c_{12} & c_{13} & c_{14} \\
                               0 & c_{22} & c_{23} & c_{24} \\
                               0 & 0      & c_{33} & c_{34} \\
                               0 & 0      &  0     & c_{44} 
             \end{array} \right).
\]
The matrix ${\bf A}\in \mathbb{R}^{n\times n}$ has an LU factorization if the determinant 
is different from zero. If the LU factorization exists and ${\bf A}$ is non-singular, then the LU factorization
is unique and the determinant is given by 
\[
det\{{\bf A}\}
  = c_{11}c_{22}\dots c_{nn}.
\]
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{How should we structure our code?}
What do you think is reasonable to split into subtasks defined by classes?
\begin{itemize}
\item Single-particle  wave functions?
\item External potentials?
\item Operations on $r_{ij}$ and the correlation function?
\item Mathematical operations like the first and second derivative of the
trial wave function? How can you split the derivatives into various subtasks?
\item Matrix and vector operations? 
\end{itemize}
Your task is to figure out how to structure your code in order
to compute the Slater determinant for beryllium. Do not include the correlation factor in the first attempt nor the electron-electron repulsion. You should also hard-code the $2\times 2$ determinant as that serves as a simple test. 
}


\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

The expectation value of the kinetic energy expressed in atomic units for electron $i$ is 
\begin{equation}
 \langle \Op{K}_i \rangle = -\frac{1}{2}\frac{\langle\Psi|\nabla_{i}^2|\Psi \rangle}{\langle\Psi|\Psi \rangle},
\end{equation}

\begin{equation}\label{kineticE}
K_i = -\frac{1}{2}\frac{\nabla_{i}^{2} \Psi}{\Psi}.
\end{equation}
\begin{eqnarray}
\frac{\nabla^2 \Psi}{\Psi} & = & \frac{\nabla^2 ({\Psi_{D} \,  \Psi_C})}{\Psi_{D} \,  \Psi_C} = \frac{\Grad \cdot [\Grad {(\Psi_{D} \,  \Psi_C)}]}{\Psi_{D} \,  \Psi_C} = \frac{\Grad \cdot [ \Psi_C \Grad \Psi_{D} + \Psi_{D} \Grad  \Psi_C]}{\Psi_{D} \,  \Psi_C}\nonumber\\
&  = & \frac{\Grad  \Psi_C \cdot \Grad \Psi_{D} +  \Psi_C \nabla^2 \Psi_{D} + \Grad \Psi_{D} \cdot \Grad  \Psi_C + \Psi_{D} \nabla^2  \Psi_C}{\Psi_{D} \,  \Psi_C}\nonumber\\
\end{eqnarray}
\begin{eqnarray}
\frac{\nabla^2 \Psi}{\Psi}
& = & \frac{\nabla^2 \Psi_{D}}{\Psi_{D}} + \frac{\nabla^2  \Psi_C}{ \Psi_C} + 2 \frac{\Grad \Psi_{D}}{\Psi_{D}}\cdot\frac{\Grad  \Psi_C}{ \Psi_C}
\end{eqnarray}
 }
 \end{small}
 }


\frame
{
  \frametitle{Summing up: Bringing it all together, Local energy}
\begin{small}
{\scriptsize

The second derivative of the Jastrow factor divided by the Jastrow factor (the way it enters the kinetic energy) is
\[
\left[\frac{\nabla^2 \Psi_C}{\Psi_C}\right]_x =\  
2\sum_{k=1}^{N}
\sum_{i=1}^{k-1}\frac{\partial^2 g_{ik}}{\partial x_k^2}\ +\ 
\sum_{k=1}^N
\left(
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k} -
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_i}
\right)^2
\]
But we have a simple form for the function, namely
\[
\Psi_{C}=\prod_{i< j}\exp{f(r_{ij})}= \exp{\left\{\sum_{i<j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
\]
and it is easy to see that for particle  $k$
we have
\[
  \frac{\nabla^2_k \Psi_C}{\Psi_C }=
\sum_{ij\ne k}\frac{({\bf r}_k-{\bf r}_i)({\bf r}_k-{\bf r}_j)}{r_{ki}r_{kj}}f'(r_{ki})f'(r_{kj})+
\sum_{j\ne k}\left( f''(r_{kj})+\frac{2}{r_{kj}}f'(r_{kj})\right)
\]
}
\end{small}
}



\frame
{
  \frametitle{Bringing it all together, Local energy}
\begin{small}
{\scriptsize
Using 
\[
f(r_{ij})= \frac{ar_{ij}}{1+\beta r_{ij}},
\]
and $g'(r_{kj})=dg(r_{kj})/dr_{kj}$ and 
$g''(r_{kj})=d^2g(r_{kj})/dr_{kj}^2$  we find that for particle  $k$
we have
\[
  \frac{\nabla^2_k \Psi_C}{\Psi_C }=
\sum_{ij\ne k}\frac{({\bf r}_k-{\bf r}_i)({\bf r}_k-{\bf r}_j)}{r_{ki}r_{kj}}\frac{a}{(1+\beta r_{ki})^2}
\frac{a}{(1+\beta r_{kj})^2}+
\sum_{j\ne k}\left(\frac{2a}{r_{kj}(1+\beta r_{kj})^2}-\frac{2a\beta}{(1+\beta r_{kj})^3}\right)
\]
}
\end{small}
}

\frame
{
  \frametitle{Local energy}
\begin{small}
{\scriptsize
The gradient and
Laplacian can be calculated as follows:
\[
\frac{\vec\nabla_i\det{\matr D(\mathbf{r})}}
{\det{\matr D(\mathbf{r})}} =
\sum_{j=1}^N \vec\nabla_i d_{ij}(\mathbf{r})\,
d_{ji}^{-1}(\mathbf{r}) =
\sum_{j=1}^N \vec\nabla_i \phi_j(\mathbf{r}_i)\,
d_{ji}^{-1}(\mathbf{r})
\]
and
\[
\frac{\nabla^2_i\det{\matr D(\mathbf{r})}}
{\det{\matr D(\mathbf{r})}} =
\sum_{j=1}^N \nabla^2_i d_{ij}(\mathbf{r})\,
d_{ji}^{-1}(\mathbf{r}) =
\sum_{j=1}^N \nabla^2_i \phi_j(\mathbf{r}_i)\,
d_{ji}^{-1}(\mathbf{r})
\]
}
\end{small}
}



\frame
{
  \frametitle{Determinant part in quantum force}
\begin{small}
{\scriptsize
The gradient for the determinant is 
\[
\frac{\vec\nabla_i\det{\matr D(\mathbf{r})}}
{\det{\matr D(\mathbf{r})}} =
\sum_{j=1}^N \vec\nabla_i d_{ij}(\mathbf{r})\,
d_{ji}^{-1}(\mathbf{r}) =
\sum_{j=1}^N \vec\nabla_i \phi_j(\mathbf{r}_i)\,
d_{ji}^{-1}(\mathbf{r}).
\]
}
\end{small}
}


\frame
{
  \frametitle{Jastrow gradient in quantum force}
\begin{small}
{\scriptsize
We have
\[
\Psi_C=\prod_{i< j}g(r_{ij})= \exp{\left\{\sum_{i<j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
\]
the gradient needed for the quantum force and local energy is easy to compute.  
We get for particle  $k$
\[
\frac{ \nabla_k \Psi_C}{ \Psi_C }= \sum_{j\ne k}\frac{{\bf r}_{kj}}{r_{kj}}\frac{a}{(1+\beta r_{kj})^2},
\]
which is rather easy to code.  Remember to sum over all particles  when you compute the local energy.
}
\end{small}
}
\frame
{
  \frametitle{Metropolis Hastings part}
\begin{small}
{\scriptsize

We need to compute the ratio between wave functions, in particular  for the Slater determinants.
\[
R =
%\frac{\det{\matr D(\mathbf{r}^{\mathrm{new}})}}
%{\det{\matr D(\mathbf{r}^{\mathrm{old}})}} =
\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{new}})\,
d_{ji}^{-1}(\mathbf{r}^{\mathrm{old}}) = 
\sum_{j=1}^N \phi_j(\mathbf{r}_i^{\mathrm{new}})\,
d_{ji}^{-1}(\mathbf{r}^{\mathrm{old}})
\]
What this means is that in order to get the ratio when only the $i$th
particle  has been moved, we only need to calculate the dot
product of the vector $\left(\phi_1(\mathbf{r}_i^\mathrm{new}),\,\dots,\,
\phi_N(\mathbf{r}_i^\mathrm{new})\right)$ of single particle  wave functions
evaluated at this new position with the $i$th column of the inverse
matrix $\matr D^{-1}$ evaluated at the original position. Such
an operation has a time scaling of $\bigO(N)$. The only extra thing we
need to do is to maintain the inverse matrix $\matr D^{-1}(\vec
x^{\mathrm{old}})$.
}
\end{small}
}



\frame
 {
   \frametitle{Jastrow factor in Metropolis Hastings}
 \begin{small}
 {\scriptsize
We have
\begin{equation}
 \boxed{R_{C} = \frac{\Psi_{C}^\mathrm{new}}{\Psi_{C}^\mathrm{cur}} = \frac{e^{U_{new}}}{e^{U_{cur}}} = e^{\Delta U},}
\end{equation}
where
\begin{equation}
\Delta U =
\sum_{i=1}^{k-1}\big(f_{ik}^\mathrm{new}-f_{ik}^\mathrm{cur}\big)
+
\sum_{i=k+1}^{N}\big(f_{ki}^\mathrm{new}-f_{ki}^\mathrm{cur}\big)
\end{equation}
One needs to develop a special algorithm 
that runs only through the elements of the upper triangular
matrix $\bfv{g}$ and have $k$ as an index. 

 }
 \end{small}
 }



\section[Week 10]{Week 10}
\frame
{
  \frametitle{Topics for Week 10, March 4-8}
  \begin{block}{Slater determinants for systems with more than two particles}
\begin{itemize}
\item Repetition from last time, discusssion of Slater determinants
\item Treatment of statistical data, blocking as a simple way to extract the standard deviation
\item Find minima in multivariable spaces, begin discussion of the Conjugate Gradient method
\end{itemize}
Project work this week: Finalize the Slater determinant part and include blocking. Start thinking of applying the conjugate gradient method. For a proof of blocking, partly to be discussed later this week, see the article of 
H.~Flyvbjerg and H.~G.~Petersen, {\em Error estimates on averages of correlated data},  The Journal of Chemical Physics {\bf 91}, 461-466 (1989).
For next week: Read this article and look at the first simple example. Look  also through chapters 10.2-10.7 of Numerical recipes on the conjugate gradient method and related methods.
  \end{block}
} 

\frame[containsverbatim]
{
  \frametitle{Useful equations}
\begin{small}
{\scriptsize
The $1s$ hydrogen like wave function
\[
R_{10}(r) =  2\left(\frac{Z}{a_0}\right)^{3/2}\exp{(-Zr/a_0)}= u_{10}/r 
\]
The total energy for helium (not the Hartree or Fock terms) from  the direct and the exchange term should give $5Z/8$.

The single-particle energy with no interactions should give $-Z^2/2n^2$. 

The $2s$ hydrogen-like wave function is
\[
R_{20}(r) =  2\left(\frac{Z}{2a_0}\right)^{3/2}\left(1-\frac{Zr}{2a_0}\right)\exp{(-Zr/2a_0)}= u_{20}/r 
\]
and the $2p$ hydrogen -like wave function is
\[
R_{21}(r) =  \frac{1}{\sqrt{3}}\left(\frac{Z}{2a_0}\right)^{3/2}\frac{Zr}{a_0}\exp{(-Zr/2a_0)}= u_{21}/r 
\]
We use $a_0=1$.
 }
 \end{small}
 }



\frame
{
  \frametitle{Problems with neon for VMC}
\begin{small}
{\scriptsize
In the standard textbook case one uses spherical coordinates  in order to get the hydrogen-like wave functions
     \[
        x=rsin\theta cos\phi,  
      \]
      \[
        y=rsin\theta sin\phi,
     \]
and
     \[
        z=rcos\theta.
     \]
The reason we introduce spherical coordinates is the spherical symmetry of the Coulomb potential
\[
    \frac{e^2}{4\pi\epsilon_0r}=\frac{e^2}{4\pi\epsilon_0\sqrt{x^2+y^2+z^2}},
\]
where we have used $r=\sqrt{x^2+y^2+z^2}$. 
It is not possible to find a separable solution of the type
\[
    \psi(x,y,z)=\psi(x)\psi(y)\psi(z).
\]
However, with spherical coordinates we can find a solution
of the form
\[
   \psi(r,\theta,\phi)=R(r)P(\theta)F(\phi).
\]
}
\end{small}
}




\frame
{
  \frametitle{Problems with neon for VMC}
\begin{small}
{\scriptsize
The angle-dependent differential equations result in the spherical harmonic functions as
solutions, with quantum numbers $l$ and $m_l$.
These functions are given by
\[
    Y_{lm_l}(\theta,\phi)=P(\theta)F(\phi)=\sqrt{\frac{(2l+1)(l-m_l)!}{4\pi (l+m_l)!}}
                      P_l^{m_l}(cos(\theta))\exp{(im_l\phi)},
\]
with $P_l^{m_l}$ being the associated Legendre polynomials
They can be rewritten as 
\[
   Y_{lm_l}(\theta,\phi)=sin^{|m_l|}(\theta) \times (\mathrm{polynom}(cos\theta))\exp{(im_l\phi)},
\]
}
\end{small}
}


\frame
{
  \frametitle{Problems with neon for VMC}
\begin{small}
{\scriptsize
We have the following selected examples
\[
   Y_{00}=\sqrt{\frac{1}{4\pi}},
\]
for $l=m_l=0$, 
\[
   Y_{10}=\sqrt{\frac{3}{4\pi}}cos(\theta),
\]
for $l=1$ og $m_l=0$, 
\[
   Y_{1\pm 1}=\sqrt{\frac{3}{8\pi}}sin(\theta)exp(\pm i\phi),
\]
for  $l=1$ og $m_l=\pm 1$. 
}
\end{small}
}



\frame
{
  \frametitle{Problems with neon for VMC}
\begin{small}
{\scriptsize
A problem with the spherical harmonics is that they are complex. The introduction of
\emph{solid harmonics} allows the use
of real orbital wave-functions for a wide range of applications. The
complex solid harmonics ${\cal Y}_{lm_l}(\mathbf{r})$ are related to
the spherical harmonics  $Y_{lm_l}(\mathbf{r})$ through
\begin{equation*}
  {\cal Y}_{lm_l}(\mathbf{r}) = r^l Y_{lm_l}(\mathbf{r}).
\end{equation*}
By factoring out the leading $r$-dependency of the radial-function
\begin{equation*}
  {\cal R}_{nl}(\mathbf{r}) = r^{-l} R_{nl}(\mathbf{r}),
\end{equation*}
we obtain 
\begin{equation*}
  \Psi_{nlm_l}(r,\theta, \phi) %=R_{nl}(r) \cdot Y_{lm_l}(\theta,\phi)
  = {\cal R}_{nl}(\mathbf{r})\cdot{\cal Y}_{lm_l}(\mathbf{r}).
%\label{totalSolidHydrogenWavefunction}
\end{equation*}
}
\end{small}
}










\frame
{
  \frametitle{Problems with neon for VMC}
\begin{small}
{\scriptsize
For the theoretical development of the \emph{real solid harmonics} we first 
express the complex solid harmonics, $C_{lm_l}$, by (complex) Cartesian
coordinates, and arrive at the real solid harmonics, $S_{lm_l}$, through
the unitary transformation
\begin{equation*}
  \left( \begin{split} &\phantom{i} S_{lm_l} \\ 
    &S_{l,-m_l} \end{split} \right) 
  = \frac{1}{\sqrt{2}} \left(        \begin{split}
    (-1)^m_l \phantom{a} & \phantom{aa} 1 \\ 
    -(-1)^m_l i & \phantom{aa} i       \end{split} \right)  
  \left( \begin{split} &\phantom{i} C_{lm_l} \\ 
    &C_{l,-m_l} \end{split} \right).
\end{equation*}
}
\end{small}
}



\frame
{
  \frametitle{Problems with neon for VMC}
\begin{small}
{\scriptsize
This transformation will not alter any physical quantities that are
degenerate in the subspace consisting of opposite magnetic quantum
numbers (the angular momentum $l$ is equal for both these cases). This
means for example that the above transformation does not alter the
energies, unless an external magnetic field is applied to the
system. Henceforth, we will use the solid harmonics, and note that
changing the spherical potential beyond the Coulomb potential will not
alter the solid harmonics.
}
\end{small}
}



\frame
{
  \frametitle{Problems with neon for VMC}
\begin{small}
{\scriptsize
We have defined 
\begin{equation*}
  {\cal Y}_{lm_l}(\mathbf{r}) = r^l Y_{lm_l}(\mathbf{r}).
\end{equation*}
The real-valued spherical harmonics are defined as
\[
S_{l0} =  \sqrt{\frac{4\pi}{2l+1}} {\cal Y}_{l0}(\mathbf{r}),
\]
\[
S_{lm_l} =  (-1)^{m_l}\sqrt{\frac{8\pi}{2l+1}} \mathrm{Re}{\cal Y}_{l0}(\mathbf{r}),
\]

\[
S_{lm_l} =  (-1)^{m_l}\sqrt{\frac{8\pi}{2l+1}} \mathrm{Im}{\cal Y}_{l0}(\mathbf{r}),
\]
for $m_l> 0$.

}
\end{small}
}




\frame
{
  \frametitle{Problems with neon for VMC}
\begin{small}
{\scriptsize
 The lowest-order real solid harmonics are
listed in here
\begin{center} {\large \bf Real Solid Harmonics} \\ 
$\phantom{a}$ \\
\begin{tabular}{ccccc}
\hline\\ 
$m_l\backslash l$ & \phantom{AA}0\phantom{AA}
& \phantom{AA}1\phantom{AA} & \phantom{AA}2\phantom{AA} &
\phantom{AA}3\phantom{AA} \\ 
\hline\\ 
+3& & &
&$\frac{1}{2}\sqrt{\frac{5}{2}}(x^2-3y^2)x$ \\ [7pt] 
+2& & &$\frac{1}{2}\sqrt{3}(x^2-y^2)$&$\frac{1}{2}\sqrt{15}(x^2-y^2)z$
\\ [7pt] 
+1& &x&$\sqrt{3}xz$
&$\frac{1}{2}\sqrt{\frac{3}{2}}(5z^2-r^2)x$ \\ [7pt] 
0&1&y&$\frac{1}{2}(3z^2-r^2)$       &$\frac{1}{2}(5z^2-3r^2)x$ \\
 [7pt] 
-1& &z&$\sqrt{3}yz$
&$\frac{1}{2}\sqrt{\frac{3}{2}}(5z^2-r^2)y$ \\ [7pt] 
-2& & &$\sqrt{3}xy$                  &$\sqrt{15}xyz$ \\ [7pt] 
-3& & &
&$\frac{1}{2}\sqrt{\frac{5}{2}}(3x^2-y^2)y$ \\ [7pt] 
\hline
\end{tabular} 
\end{center}

}
\end{small}
}





\frame{
  \frametitle{Why blocking?}
  \begin{block}{Statistical analysis}
    \begin{itemize}
    \item Monte Carlo simulations can be treated as {\em computer
      experiments}
    \item The results can be analysed with the same statistical tools as
      we would use analysing experimental data.
    \item As in all experiments, we are looking for expectation
      values and an estimate of how accurate they are, i.e., possible sources for errors.
    \end{itemize}
  \end{block}
}

\frame{
  \frametitle{Why blocking?}
  \begin{block}{Statistical analysis}
    \begin{itemize}
    \item As in other experiments, Monte Carlo experiments have two
      classes of errors:
      \begin{itemize}
      \item Statistical errors
      \item Systematical errors
      \end{itemize}
    \item Statistical errors can be estimated using standard tools
      from statistics
    \item Systematical errors are method specific and must be treated
      differently from case to case. (In VMC a common source is the
      step length or time step in importance sampling)
    \end{itemize}
  \end{block}
}




\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
The \emph{probability distribution function (PDF)} is a function
$p(x)$ on the domain which, in the discrete case, gives us the
probability or relative frequency with which these values of $X$
occur:
\bdm
p(x) = \prob(X=x)
\edm
In the continuous case, the PDF does not directly depict the
actual probability. Instead we define the probability for the
stochastic variable to assume any value on an infinitesimal interval
around $x$ to be $p(x)dx$. The continuous function $p(x)$ then gives us
the \emph{density} of the probability rather than the probability
itself. The probability for a stochastic variable to assume any value
on a non-infinitesimal interval $[a,\,b]$ is then just the integral:
\bdm
\prob(a\leq X\leq b) = \int_a^b p(x)dx
\edm
Qualitatively speaking, a stochastic variable represents the values of
numbers chosen as if by chance from some specified PDF so that the
selection of a large set of these numbers reproduces this PDF.
}
\end{small}
}


\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
Also of interest to us is the \emph{cumulative probability
distribution function (CDF)}, $P(x)$, which is just the probability
for a stochastic variable $X$ to assume any value less than $x$:
\bdm
P(x)=\mathrm{Prob(}X\leq x\mathrm{)} =
\int_{-\infty}^x p(x^{\prime})dx^{\prime}
\edm
The relation between a CDF and its corresponding PDF is then:
\bdm
p(x) = \frac{d}{dx}P(x)
\edm
}
\end{small}
}


\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
A particularly useful class of special expectation values are the
\emph{moments}. The $n$-th moment of the PDF $p$ is defined as
follows:
\bdm
\mean{x^n} \equiv \int\! x^n p(x)\,dx
\edm
The zero-th moment $\mean{1}$ is just the normalization condition of
$p$. The first moment, $\mean{x}$, is called the \emph{mean} of $p$
and often denoted by the letter $\mu$:
\bdm
\mean{x} = \mu \equiv \int\! x p(x)\,dx
\edm
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
A special version of the moments is the set of \emph{central moments},
the n-th central moment defined as:
\bdm
\mean{(x-\mean{x})^n} \equiv \int\! (x-\mean{x})^n p(x)\,dx
\edm
The zero-th and first central moments are both trivial, equal $1$ and
$0$, respectively. But the second central moment, known as the
\emph{variance} of $p$, is of particular interest. For the stochastic
variable $X$, the variance is denoted as $\sigma^2_X$ or $\var(X)$:
\beaN
\sigma^2_X\ \ =\ \ \var(X) & = & \mean{(x-\mean{x})^2} =
\int\! (x-\mean{x})^2 p(x)\,dx\\
& = & \int\! \left(x^2 - 2 x \mean{x}^{\phantom{2}} +
  \mean{x}^2\right)p(x)\,dx\\
& = & \mean{x^2} - 2 \mean{x}\mean{x} + \mean{x}^2\\
& = & \mean{x^2} - \mean{x}^2
\eeaN
The square root of the variance, $\sigma =
\sqrt{\mean{(x-\mean{x})^2}}$ is called the \emph{standard
  deviation} of $p$. It is clearly just the RMS (root-mean-square)
value of the deviation of the PDF from its mean value, interpreted
qualitatively as the ``spread'' of $p$ around its mean.
}
\end{small}
}


\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
Another important quantity is the so called covariance, a variant of
the above defined variance. Consider again the set $\{X_i\}$ of $n$
stochastic variables (not necessarily uncorrelated) with the
multivariate PDF $P(x_1,\dots,x_n)$. The \emph{covariance} of two
of the stochastic variables, $X_i$ and $X_j$, is defined as follows:
\bea
\cov(X_i,\,X_j) &\equiv& \meanb{(x_i-\mean{x_i})(x_j-\mean{x_j})}
\nonumber\\
&=&
\int\!\cdots\!\int\!(x_i-\mean{x_i})(x_j-\mean{x_j})\,
P(x_1,\dots,x_n)\,dx_1\dots dx_n
\label{eq:def_covariance}
\eea
with
\bdm
\mean{x_i} =
\int\!\cdots\!\int\!x_i\,P(x_1,\dots,x_n)\,dx_1\dots dx_n
\edm
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
If we consider the above covariance as a matrix $C_{ij} =
\cov(X_i,\,X_j)$, then the diagonal elements are just the familiar
variances, $C_{ii} = \cov(X_i,\,X_i) = \var(X_i)$. It turns out that
all the off-diagonal elements are zero if the stochastic variables are
uncorrelated. This is easy to show, keeping in mind the linearity of
the expectation value. Consider the stochastic variables $X_i$ and
$X_j$, ($i\neq j$):
\beaN
\cov(X_i,\,X_j) &=& \meanb{(x_i-\mean{x_i})(x_j-\mean{x_j})}\\
&=&\mean{x_i x_j - x_i\mean{x_j} - \mean{x_i}x_j + \mean{x_i}\mean{x_j}}\\
&=&\mean{x_i x_j} - \mean{x_i\mean{x_j}} - \mean{\mean{x_i}x_j} +
\mean{\mean{x_i}\mean{x_j}}\\
&=&\mean{x_i x_j} - \mean{x_i}\mean{x_j} - \mean{x_i}\mean{x_j} +
\mean{x_i}\mean{x_j}\\
&=&\mean{x_i x_j} - \mean{x_i}\mean{x_j}
\eeaN
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
If $X_i$ and $X_j$ are independent, we get $\mean{x_i x_j} =
\mean{x_i}\mean{x_j}$, resulting in $\cov(X_i, X_j) = 0\ \ (i\neq j)$.

Also useful for us is the covariance of linear combinations of
stochastic variables. Let $\{X_i\}$ and $\{Y_i\}$ be two sets of
stochastic variables. Let also $\{a_i\}$ and $\{b_i\}$ be two sets of
scalars. Consider the linear combination:
\bdm
U = \sum_i a_i X_i \qquad V = \sum_j b_j Y_j
\edm
By the linearity of the expectation value
\bdm
\cov(U, V) = \sum_{i,j}a_i b_j \cov(X_i, Y_j)
\edm
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
Now, since the variance is just $\var(X_i) = \cov(X_i, X_i)$, we get
the variance of the linear combination $U = \sum_i a_i X_i$:
\be
\var(U) = \sum_{i,j}a_i a_j \cov(X_i, X_j)
\label{eq:variance_linear_combination}
\ee
And in the special case when the stochastic variables are
uncorrelated, the off-diagonal elements of the covariance are as we
know zero, resulting in:
\bdm
\var(U) = \sum_i a_i^2 \cov(X_i, X_i) = \sum_i a_i^2 \var(X_i)
\edm
\bdm
\var(\sum_i a_i X_i) = \sum_i a_i^2 \var(X_i)
\edm
which will become very useful in our study of the error in the mean
value of a set of measurements.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
A \emph{stochastic process} is a process that produces sequentially a
chain of values:
\bdm
\{x_1, x_2,\dots\,x_k,\dots\}.
\edm
We will call these
values our \emph{measurements} and the entire set as our measured
\emph{sample}.  The action of measuring all the elements of a sample
we will call a stochastic \emph{experiment} (since, operationally,
they are often associated with results of empirical observation of
some physical or mathematical phenomena; precisely an experiment). We
assume that these values are distributed according to some 
PDF $p_X^{\phantom X}(x)$, where $X$ is just the formal symbol for the
stochastic variable whose PDF is $p_X^{\phantom X}(x)$. Instead of
trying to determine the full distribution $p$ we are often only
interested in finding the few lowest moments, like the mean
$\mu_X^{\phantom X}$ and the variance $\sigma_X^{\phantom X}$.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
In practical situations a sample is always of finite size. Let that
size be $n$. The expectation value of a sample, the \emph{sample
mean}, is then defined as follows:
\bdm
\bar x_n \equiv \frac{1}{n}\sum_{k=1}^n x_k
\edm
The \emph{sample variance} is:
\bdm
\var(x) \equiv \frac{1}{n}\sum_{k=1}^n (x_k - \bar x_n)^2
\edm
its square root being the \emph{standard deviation of the sample}. The
\emph{sample covariance} is:
\bdm
\cov(x)\equiv\frac{1}{n}\sum_{kl}(x_k - \bar x_n)(x_l - \bar x_n)
\edm
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
Note that the sample variance is the sample covariance without the
cross terms. In a similar manner as the covariance in
eq.~(\ref{eq:def_covariance}) is a measure of the correlation between
two stochastic variables, the above defined sample covariance is a
measure of the sequential correlation between succeeding measurements
of a sample.

These quantities, being known experimental values, differ
significantly from and must not be confused with the similarly named
quantities for stochastic variables, mean $\mu_X$, variance $\var(X)$
and covariance $\cov(X,Y)$.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
The law of large numbers
states that as the size of our sample grows to infinity, the sample
mean approaches the true mean $\mu_X^{\phantom X}$ of the chosen PDF:
\bdm
\lim_{n\to\infty}\bar x_n = \mu_X^{\phantom X}
\edm
The sample mean $\bar x_n$ works therefore as an estimate of the true
mean $\mu_X^{\phantom X}$.

What we need to find out is how good an approximation $\bar x_n$ is to
$\mu_X^{\phantom X}$. In any stochastic measurement, an estimated
mean is of no use to us without a measure of its error. A quantity
that tells us how well we can reproduce it in another experiment. We
are therefore interested in the PDF of the sample mean itself. Its
standard deviation will be a measure of the spread of sample means,
and we will simply call it the \emph{error} of the sample mean, or
just sample error, and denote it by $\mathrm{err}_X^{\phantom X}$. In
practice, we will only be able to produce an \emph{estimate} of the
sample error since the exact value would require the knowledge of the
true PDFs behind, which we usually do not have.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
The straight forward brute force way of estimating the sample error is
simply by producing a number of samples, and treating the mean of each
as a measurement. The standard deviation of these means will then be
an estimate of the original sample error. If we are unable to produce
more than one sample, we can split it up sequentially into smaller
ones, treating each in the same way as above. This procedure is known
as \emph{blocking} and will be given more attention shortly. At this
point it is worth while exploring more indirect methods of estimation
that will help us understand some important underlying principles of
correlational effects.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
Let us first take a look at what happens to the sample error as the
size of the sample grows. In a sample, each of the measurements $x_i$
can be associated with its own stochastic variable $X_i$. The
stochastic variable $\overline X_n$ for the sample mean $\bar x_n$ is
then just a linear combination, already familiar to us:
\bdm
\overline X_n = \frac{1}{n}\sum_{i=1}^n X_i
\edm
All the coefficients are just equal $1/n$. The PDF of $\overline X_n$,
denoted by $p_{\overline X_n}(x)$ is the desired PDF of the sample
means. 
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
The probability density of obtaining a sample mean $\bar x_n$
is the product of probabilities of obtaining arbitrary values $x_1,
x_2,\dots,x_n$ with the constraint that the mean of the set $\{x_i\}$
is $\bar x_n$:
\bdm
p_{\overline X_n}(x) = \int p_X^{\phantom X}(x_1)\cdots
\int p_X^{\phantom X}(x_n)\ 
\delta\!\left(x - \frac{x_1+x_2+\dots+x_n}{n}\right)dx_n \cdots dx_1
\edm
And in particular we are interested in its variance $\var(\overline
X_n)$.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
It is generally not possible to express $p_{\overline X_n}(x)$ in a
closed form given an arbitrary PDF $p_X^{\phantom X}$ and a number
$n$. But for the limit $n\to\infty$ it is possible to make an
approximation. The very important result is called \emph{the central
  limit theorem}. It tells us that as $n$ goes to infinity,
$p_{\overline X_n}(x)$ approaches a Gaussian distribution whose mean
and variance equal the true mean and variance, $\mu_{X}^{\phantom X}$
and $\sigma_{X}^{2}$, respectively:
\be
\lim_{n\to\infty} p_{\overline X_n}(x) =
\left(\frac{n}{2\pi\var(X)}\right)^{1/2}
e^{-\frac{n(x-\bar x_n)^2}{2\var(X)}}
\label{eq:central_limit_gaussian}
\ee
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
The desired variance
$\var(\overline X_n)$, i.e. the sample error squared
$\mathrm{err}_X^2$, is given by:
\be
\mathrm{err}_X^2 = \var(\overline X_n) = \frac{1}{n^2}
\sum_{ij} \cov(X_i, X_j)
\label{eq:error_exact}
\ee
We see now that in order to calculate the exact error of the sample
with the above expression, we would need the true means
$\mu_{X_i}^{\phantom X}$ of the stochastic variables $X_i$. To
calculate these requires that we know the true multivariate PDF of all
the $X_i$. But this PDF is unknown to us, we have only got the measurements of
one sample. The best we can do is to let the sample itself be an
estimate of the PDF of each of the $X_i$, estimating all properties of
$X_i$ through the measurements of the sample.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
Our estimate of $\mu_{X_i}^{\phantom X}$ is then the sample mean $\bar x$
itself, in accordance with the the central limit theorem:
\bdm
\mu_{X_i}^{\phantom X} = \mean{x_i} \approx \frac{1}{n}\sum_{k=1}^n
x_k = \bar x
\edm
Using $\bar x$ in place of $\mu_{X_i}^{\phantom X}$ we can give an
\emph{estimate} of the covariance in eq.~(\ref{eq:error_exact}):
\beaN
\cov(X_i, X_j) &=& \mean{(x_i-\mean{x_i})(x_j-\mean{x_j})}
\approx\mean{(x_i - \bar x)(x_j - \bar{x})}\\
&\approx&\frac{1}{n} \sum_{l}^n \left(\frac{1}{n}\sum_{k}^n (x_k -
\bar x_n)(x_l - \bar x_n)\right)
=\frac{1}{n}\frac{1}{n} \sum_{kl} (x_k -
\bar x_n)(x_l - \bar x_n)\\
&=&\frac{1}{n}\cov(x)
\eeaN
}
\end{small}
}



\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
By the same procedure we can use the sample variance as an
estimate of the variance of any of the stochastic variables $X_i$:
\bea
\var(X_i)
&=&\mean{x_i - \mean{x_i}} \approx \mean{x_i - \bar x_n}\nonumber\\
&\approx&\frac{1}{n}\sum_{k=1}^n (x_k - \bar x_n)\nonumber\\
&=&\var(x)
\label{eq:var_estimate_i_think}
\eea
Now we can calculate an estimate of the error
$\mathrm{err}_X^{\phantom X}$ of the sample mean $\bar x_n$:
\bea
\mathrm{err}_X^2
&=&\frac{1}{n^2}\sum_{ij} \cov(X_i, X_j) \nonumber \\
&\approx&\frac{1}{n^2}\sum_{ij}\frac{1}{n}\cov(x) =
\frac{1}{n^2}n^2\frac{1}{n}\cov(x)\nonumber\\
&=&\frac{1}{n}\cov(x)
\label{eq:error_estimate}
\eea
which is nothing but the sample covariance divided by the number of
measurements in the sample.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
In the special case that the measurements of the sample are
uncorrelated (equivalently the stochastic variables $X_i$ are
uncorrelated) we have that the off-diagonal elements of the covariance
are zero. This gives the following estimate of the sample error:
\bea
\mathrm{err}_X^2 &=& \frac{1}{n^2}\sum_{ij} \cov(X_i, X_j) =
\frac{1}{n^2} \sum_i \var(X_i)\nonumber\\
&\approx&
\frac{1}{n^2} \sum_i \var(x)\nonumber\\ &=& \frac{1}{n}\var(x)
\label{eq:error_estimate_uncorrel}
\eea
where in the second step we have used eq.~(\ref{eq:var_estimate_i_think}).
The error of the sample is then just its standard deviation divided by
the square root of the number of measurements the sample contains.
This is a very useful formula which is easy to compute. It acts as a
first approximation to the error, but in numerical experiments, we
cannot overlook the always present correlations.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
For computational purposes one usually splits up the estimate of
$\mathrm{err}_X^2$, given by eq.~(\ref{eq:error_estimate}), into two
parts:
\bea
\mathrm{err}_X^2 &=&
\frac{1}{n}\var(x) + \frac{1}{n}(\cov(x)-\var(x))\nonumber\\&=&
\frac{1}{n^2}\sum_{k=1}^n (x_k - \bar x_n)^2 +
\frac{2}{n^2}\sum_{k<l} (x_k - \bar x_n)(x_l - \bar x_n)
\label{eq:error_estimate_split_up}
\eea
The first term is the same as the error in the uncorrelated case,
eq.~(\ref{eq:error_estimate_uncorrel}). This means that the second
term accounts for the error correction due to correlation between the
measurements. For uncorrelated measurements this second term is zero.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
Computationally the uncorrelated first term is much easier to treat
efficiently than the second.
\bdm
\var(x) = \frac{1}{n}\sum_{k=1}^n (x_k - \bar x_n)^2 =
\left(\frac{1}{n}\sum_{k=1}^n x_k^2\right) - \bar x_n^2
\edm
We just accumulate separately the values $x^2$ and $x$ for every
measurement $x$ we receive. The correlation term, though, has to be
calculated at the end of the experiment since we need all the
measurements to calculate the cross terms. Therefore, all measurements
have to be stored throughout the experiment.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
Let us analyze the problem by splitting up the correlation term into
partial sums of the form:
\bdm
f_d = \frac{1}{n-d}\sum_{k=1}^{n-d}(x_k - \bar x_n)(x_{k+d} - \bar x_n)
\edm
The correlation term of the error can now be rewritten in terms of
$f_d$:
\bdm
\frac{2}{n}\sum_{k<l} (x_k - \bar x_n)(x_l - \bar x_n) =
2\sum_{d=1}^{n-1} f_d
\edm
The value of $f_d$ reflects the correlation between measurements
separated by the distance $d$ in the sample samples.  Notice that for
$d=0$, $f$ is just the sample variance, $\var(x)$. If we divide $f_d$
by $\var(x)$, we arrive at the so called \emph{autocorrelation
  function}:
\bdm
\kappa_d = \frac{f_d}{\var(x)}
\edm
which gives us a useful measure of the correlation pair correlation
starting always at $1$ for $d=0$.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
The sample error (see eq.~(\ref{eq:error_estimate_split_up})) can now be
written in terms of the autocorrelation function:
\bea
\mathrm{err}_X^2 &=&
\frac{1}{n}\var(x)+\frac{2}{n}\cdot\var(x)\sum_{d=1}^{n-1}
\frac{f_d}{\var(x)}\nonumber\\ &=&
\left(1+2\sum_{d=1}^{n-1}\kappa_d\right)\frac{1}{n}\var(x)\nonumber\\
&=&\rule{0pt}{17pt}
\frac{\tau}{n}\cdot\var(x)
\label{eq:error_estimate_corr_time}
\eea
and we see that $\mathrm{err}_X$ can be expressed in terms the
uncorrelated sample variance times a correction factor $\tau$ which
accounts for the correlation between measurements. We call this
correction factor the \emph{autocorrelation time}:
\be
\tau = 1+2\sum_{d=1}^{n-1}\kappa_d
\label{eq:autocorrelation_time}
\ee
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
For a correlation free experiment, $\tau$
equals 1. From the point of view of
eq.~(\ref{eq:error_estimate_corr_time}) we can interpret a sequential
correlation as an effective reduction of the number of measurements by
a factor $\tau$. The effective number of measurements becomes:
\bdm
n_\mathrm{eff} = \frac{n}{\tau}
\edm
To neglect the autocorrelation time $\tau$ will always cause our
simple uncorrelated estimate of $\mathrm{err}_X^2\approx \var(x)/n$ to
be less than the true sample error. The estimate of the error will be
too ``good''. On the other hand, the calculation of the full
autocorrelation time poses an efficiency problem if the set of
measurements is very large.
}
\end{small}
}




\frame
{
  \frametitle{Can we understand this? Time Auto-correlation Function}
\begin{small}
{\scriptsize
The so-called time-displacement autocorrelation $\phi(t)$ for a quantity ${\cal M}$ is given by
\[
\phi(t) = \int dt' \left[{\cal M}(t')-\langle {\cal M} \rangle\right]\left[{\cal M}(t'+t)-\langle {\cal M} \rangle\right],
\]
which can be rewritten as 
\[
\phi(t) = \int dt' \left[{\cal M}(t'){\cal M}(t'+t)-\langle {\cal M} \rangle^2\right],
\]
where $\langle {\cal M} \rangle$ is the average value and
${\cal M}(t)$ its instantaneous value. We can discretize this function as follows, where we used our
set of computed values ${\cal M}(t)$ for a set of discretized times (our Monte Carlo cycles corresponding to moving all electrons?)
\[
\phi(t)  = \frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}{\cal M}(t'){\cal M}(t'+t)
-\frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}{\cal M}(t')\times
\frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}{\cal M}(t'+t).\label{eq:phitf}
\]
}
\end{small}
}


\frame
{
  \frametitle{Time Auto-correlation Function}
\begin{small}
{\scriptsize
One should be careful with times close to $t_{\mathrm{max}}$, the upper limit of the sums 
becomes small and we end up integrating over a rather small time interval. This means that the statistical
error in $\phi(t)$ due to the random nature of the fluctuations in ${\cal M}(t)$ can become large.

One should therefore choose $t \ll t_{\mathrm{max}}$.

Note that the variable ${\cal M}$ can be any expectation values of interest.



The time-correlation function gives a measure of the correlation between the various values of the variable 
at a time $t'$ and a time $t'+t$. If we multiply the values of ${\cal M}$ at these two different times,
we will get a positive contribution if they are fluctuating in the same direction, or a negative value
if they fluctuate in the opposite direction. If we then integrate over time, or use the discretized version of, the time correlation function $\phi(t)$ should take a non-zero value if the fluctuations are 
correlated, else it should gradually go to zero. For times a long way apart 
the different values of ${\cal M}$  are most likely 
uncorrelated and $\phi(t)$ should be zero.
}
\end{small}
}




\frame
{
  \frametitle{Time Auto-correlation Function}
\begin{small}
{\scriptsize
We can derive the correlation time by observing that our Metropolis algorithm is based on a random
walk in the space of all  possible spin configurations. 
Our probability 
distribution function ${\bf \hat{w}}(t)$ after a given number of time steps $t$ could be written as
\[
   {\bf \hat{w}}(t) = {\bf \hat{W}^t\hat{w}}(0),
\]
with ${\bf \hat{w}}(0)$ the distribution at $t=0$ and ${\bf \hat{W}}$ representing the 
transition probability matrix. 
We can always expand ${\bf \hat{w}}(0)$ in terms of the right eigenvectors of 
${\bf \hat{v}}$ of ${\bf \hat{W}}$ as 
\[
    {\bf \hat{w}}(0)  = \sum_i\alpha_i{\bf \hat{v}}_i,
\]
resulting in 
\[
   {\bf \hat{w}}(t) = {\bf \hat{W}}^t{\bf \hat{w}}(0)={\bf \hat{W}}^t\sum_i\alpha_i{\bf \hat{v}}_i=
\sum_i\lambda_i^t\alpha_i{\bf \hat{v}}_i,
\]
with $\lambda_i$ the $i^{\mathrm{th}}$ eigenvalue corresponding to  
the eigenvector ${\bf \hat{v}}_i$. 
}
\end{small}
}



\frame
{
  \frametitle{Time Auto-correlation Function}
\begin{small}
{\scriptsize
If we assume that $\lambda_0$ is the largest eigenvector we see that in the limit $t\rightarrow \infty$,
${\bf \hat{w}}(t)$ becomes proportional to the corresponding eigenvector 
${\bf \hat{v}}_0$. This is our steady state or final distribution. 

We can relate this property to an observable like the mean energy.
With the probabilty ${\bf \hat{w}}(t)$ (which in our case is the squared trial wave function) we
can write the expectation values as 
\[
 \langle {\cal M}(t) \rangle  = \sum_{\mu} {\bf \hat{w}}(t)_{\mu}{\cal M}_{\mu},
\]  
or as the scalar of a  vector product
 \[
 \langle {\cal M}(t) \rangle  = {\bf \hat{w}}(t){\bf m},
\]  
with ${\bf m}$ being the vector whose elements are the values of ${\cal M}_{\mu}$ in its 
various microstates $\mu$.
}
\end{small}
}


\frame
{
  \frametitle{Time Auto-correlation Function}
\begin{small}
{\scriptsize
We rewrite this relation  as
 \[
 \langle {\cal M}(t) \rangle  = {\bf \hat{w}}(t){\bf m}=\sum_i\lambda_i^t\alpha_i{\bf \hat{v}}_i{\bf m}_i.
\]  
If we define $m_i={\bf \hat{v}}_i{\bf m}_i$ as the expectation value of
${\cal M}$ in the $i^{\mathrm{th}}$ eigenstate we can rewrite the last equation as
 \[
 \langle {\cal M}(t) \rangle  = \sum_i\lambda_i^t\alpha_im_i.
\] 
Since we have that in the limit $t\rightarrow \infty$ the mean value is dominated by the 
the largest eigenvalue $\lambda_0$, we can rewrite the last equation as
 \[
 \langle {\cal M}(t) \rangle  = \langle {\cal M}(\infty) \rangle+\sum_{i\ne 0}\lambda_i^t\alpha_im_i.
\] 
We define the quantity
\[
   \tau_i=-\frac{1}{log\lambda_i},
\]
and rewrite the last expectation value as
 \[
 \langle {\cal M}(t) \rangle  = \langle {\cal M}(\infty) \rangle+\sum_{i\ne 0}\alpha_im_ie^{-t/\tau_i}.
\label{eq:finalmeanm}
\] 
}
\end{small}
}


\frame
{
  \frametitle{Time Auto-correlation Function}
\begin{small}
{\scriptsize
The quantities $\tau_i$ are the correlation times for the system. They control also the auto-correlation function 
discussed above.  The longest correlation time is obviously given by the second largest
eigenvalue $\tau_1$, which normally defines the correlation time discussed above. For large times, this is the 
only correlation time that survives. If higher eigenvalues of the transition matrix are well separated from 
$\lambda_1$ and we simulate long enough,  $\tau_1$ may well define the correlation time. 
In other cases we may not be able to extract a reliable result for $\tau_1$. 
Coming back to the time correlation function $\phi(t)$ we can present a more general definition in terms
of the mean magnetizations $ \langle {\cal M}(t) \rangle$. Recalling that the mean value is equal 
to $ \langle {\cal M}(\infty) \rangle$ we arrive at the expectation values
\[
\phi(t) =\langle {\cal M}(0)-{\cal M}(\infty)\rangle \langle {\cal M}(t)-{\cal M}(\infty)\rangle,
\]
resulting in
\[
\phi(t) =\sum_{i,j\ne 0}m_i\alpha_im_j\alpha_je^{-t/\tau_i},
\]
which is appropriate for all times.
}
\end{small}
}



\frame
{
  \frametitle{Correlation Time}
\begin{small}
{\scriptsize
If the correlation function decays exponentially
\[ \phi (t) \sim \exp{(-t/\tau)}\]
then the exponential correlation time can be computed as the average
\[   \tau_{\mathrm{exp}}  =  -\langle  \frac{t}{log|\frac{\phi(t)}{\phi(0)}|} \rangle. \]

If the decay is exponential, then
\[  \int_0^{\infty} dt \phi(t)  = \int_0^{\infty} dt \phi(0)\exp{(-t/\tau)}  = \tau \phi(0),\] 
which  suggests another measure of correlation
\[   \tau_{\mathrm{int}} = \sum_k \frac{\phi(k)}{\phi(0)}, \]
called the integrated correlation time.
}
\end{small}
}







\frame{
  \frametitle{What is blocking?}
  \begin{block}{Blocking}
    \begin{itemize}
%    \item Blocking is a basic tool for estimating statistical errors
%      (Morten will talk about more tools later)
    \item Say that we have a set of samples from a Monte Carlo
      experiment
    \item Assuming (wrongly) that our samples are uncorrelated our
      best estimate of the standard deviation of the mean $\langle {\cal M}\rangle$ is given
      by
      \begin{equation*}
        \sigma=\sqrt{\frac{1}{n}\left(\langle {\cal M}^2\rangle-\langle {\cal M}\rangle^2\right)} 
      \end{equation*}
    \item If the samples are correlated we can rewrite our results to show  that
      \begin{equation*}
        \sigma=\sqrt{\frac{1+2\tau/\Delta
            t}{n}\left(\langle {\cal M}^2\rangle-\langle {\cal M}\rangle^2\right)}
      \end{equation*}
      where $\tau$ is the correlation time (the time between a sample
      and the next uncorrelated sample) and $\Delta t$ is time between
      each sample
    \end{itemize}
  \end{block}
}

\frame{
  \frametitle{What is blocking?}
  \begin{block}{Blocking}
    \begin{itemize}
    \item If $\Delta t\gg\tau$ our first estimate of $\sigma$ still holds
    \item Much more common that $\Delta t<\tau$
    \item In the method of data blocking we divide the sequence of
      samples into blocks
    \item We then take the mean $\langle {\cal M}_i\rangle$ of block $i=1\ldots
      n_{blocks}$ to calculate the total mean and variance
    \item The size of each block must be so large that sample $j$ of
      block $i$ is not correlated with sample $j$ of block $i+1$
    \item The correlation time $\tau$ would be a good choice
    \end{itemize}
  \end{block}
}

\frame{
  \frametitle{What is blocking?}
  \begin{block}{Blocking}
    \begin{itemize}
    \item Problem: We don't know $\tau$ or it is too expensive to compute
    \item Solution: Make a plot of std. dev. as a function of block
      size
    \item The estimate of std. dev. of correlated data is too low $\to$
      the error will increase with increasing block size until the
      blocks are uncorrelated, where we reach a plateau
    \item When the std. dev. stops increasing the blocks are uncorrelated
    \end{itemize}
  \end{block}
}

\frame{
  \frametitle{Implementation}
  \begin{block}{Main ideas}
    \begin{itemize}
    \item Do a parallel Monte Carlo simulation, storing all samples to
      files (one per process)
    \item Do the statistical analysis on these files, independently of
      your Monte Carlo program
    \item Read the files into an array
    \item Loop over various block sizes
    \item For each block size $n_b$, loop over the array in steps of
      $n_b$ taking the mean of elements $i 
n_b,\ldots,(i+1) n_b$
    \item Take the mean and variance of the resulting array
    \item Write the results for each block size to file for later
      analysis
    \end{itemize}
  \end{block}
}



\frame{
  \frametitle{Implementation}
  \begin{block}{Parallel file output}
    \begin{itemize}
    \item The total number of samples from all processes may get very
      large
    \item Hence, storing all samples on the master node is not a
      scalable solution
    \item Instead we store the samples from each process in separate
      files
    \item Must make sure these files have different names
    \end{itemize}
  \end{block}
  \begin{block}{String handling}
    \lstinputlisting{ostringstream.cpp}
  \end{block}
}

\frame{
  \frametitle{Implementation}
  \begin{block}{Parallel file output}
    \begin{itemize}
    \item Having separated the filenames it's just a matter of
      taking the samples and store them to file
    \item Note that there is no need for communication between the
      processes in this procedure
    \end{itemize}
  \end{block}
  \begin{block}{File dumping}
    \lstinputlisting{binaryout.cpp}
  \end{block}
}

\lstset{basicstyle=\tiny}
\frame{
  \frametitle{Implementation}
  \begin{block}{Reading the files}
    \begin{itemize}
    \item Reading the files is only about mirroring the output
    \item To make life easier for ourselves we find the filesize, and
      hence the number of samples by using the C function \texttt{stat}
    \end{itemize}
  \begin{block}{File loading}
    \lstinputlisting{binaryin.cpp}
  \end{block}
  \end{block}
}
\lstset{basicstyle=\small}

\frame{
  \frametitle{Implementation}
  \begin{block}{Blocking}
    \begin{itemize}
      \item Loop over block sizes $i
      n_b,\ldots,(i+1) n_b$
    \end{itemize}
  \end{block}
  \begin{block}{Loop over block sizes}
    \lstinputlisting{blocksizeloop.cpp}
  \end{block}
}

\frame{
  \frametitle{Implementation}
  \begin{block}{Blocking}
    \begin{itemize}
    \item The blocking itself is now just a matter of finding the
      number of blocks (note the integer division) and taking the mean of
      each block 
    \item Note the pointer aritmetic: Adding a number $i$ to an array
      pointer moves the pointer to element $i$ in the array
    \end{itemize}
  \end{block}
  \begin{block}{Blocking function}
    \lstinputlisting{blocking.cpp}
  \end{block}
}

\section[Week 14]{Week 14}
\frame
{
  \frametitle{Topics for Week 14, April 1-5}
  \begin{block}{Conjugate gradient method and molecules}
\begin{itemize}
\item Find minima in multivariable spaces, Conjugate Gradient and steepest descent methods
\item Studies of molecules, H$_2$ molecule.
\end{itemize}
  \end{block}
} 



\frame
{
  \frametitle{Conjugate gradient (CG) method}
\begin{small}
{\scriptsize
The success of the CG method  for finding solutions of non-linear problems is based
on the theory of conjugate gradients for linear systems of equations. It belongs
to the class of iterative methods for solving problems from linear algebra of the type
\[
  \hat{{\bf A}}\hat{\bf {x}} = \hat{\bf {b}}.
\]
In the iterative process we end up with a problem like
\[
  \hat{\bf {r}}= \hat{\bf {b}}-\hat{{\bf A}}\hat{\bf {x}},
\]
where $\hat{\bf {r}}$ is the so-called residual or error in the iterative process.
}
\end{small}
}


\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
The residual is zero when we reach the minimum of the quadratic equation
\[
  P(\hat{\bf {x}})=\frac{1}{2}\hat{\bf {x}}^T\hat{{\bf A}}\hat{\bf {x}} - \hat{\bf {x}}^T\hat{\bf {b}},
\]
with the constraint that the matrix $\hat{{\bf A}}$ is positive definite and symmetric.
If we search for a minimum of the quantum mechanical  variance, then the matrix 
$\hat{{\bf A}}$, which is called the Hessian, is given by the second-derivative of the variance.  This quantity is always positive definite. If we vary the energy, the Hessian may not always be positive definite. 
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple example and demonstration}
\begin{small}
{\scriptsize
For the harmonic oscillator in one-dimension with a trial wave function and probability
\[
\psi_T(x) = e^{-\alpha^2 x^2} \qquad ,P_T(x)dx = \frac{e^{-2\alpha^2 x^2}dx}{\int dx e^{-2\alpha^2 x^2}}
\]
with $\alpha$ as the variational parameter. 
We have the following local energy
\[
E_L[\alpha] = \alpha^2+x^2\left(\frac{1}{2}-2\alpha^2\right),
\]
which results in the expectation value
\[
\langle  E_L[\alpha]\rangle = \frac{1}{2}\alpha^2+\frac{1}{8\alpha^2}
\]
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple example and demonstration}
\begin{small}
{\scriptsize
The derivative of the energy with respect to $\alpha$ gives 
\[
\frac{d\langle  E_L[\alpha]\rangle}{d\alpha} = \alpha-\frac{1}{4\alpha^3}
\]
and a second derivative which is always positive (meaning that we find a minimum)
\[
\frac{d^2\langle  E_L[\alpha]\rangle}{d\alpha^2} = 1+\frac{3}{4\alpha^4}
\]
The condition 
\[
\frac{d\langle  E_L[\alpha]\rangle}{d\alpha} = 0,
\]
gives the optimal $\alpha=1/\sqrt{2}$.
}
\end{small}
}






\frame[containsverbatim]
{
  \frametitle{Simple example and demonstration}
\begin{small}
{\scriptsize
In general we end up computing the expectation value of the energy in terms 
of some parameters $\mathbf{\alpha}=\{\alpha_0,\alpha_1,\dots,\alpha_n\right})$
and we search for a minimum in parameter space.  
This leads to an energy minimization problem.

The elements of the gradient are ($Ei$ 
is the first derivative wrt to the variational parameter $\alpha_i$)
\beq
\Ei
&\!\!=&\!\! \left\langle \psiibypsi \EL + { H \psii \over \psi}
-2 \Ebar \psiibypsi \right\rangle
\label{first_deriv_nonherm}
\\
&\!\!=&\!\! 2\left\langle \psiibypsi (\EL - \Ebar) \right\rangle
\;\;\;\; {\rm (by\;Hermiticity)}.
\label{first_deriv}
\eeq
For our simple model we get the same expression for the first 
derivative (check it!).
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Simple example and demonstration}
\begin{small}
{\scriptsize
Taking the second derivative the Hessian is
\beq
\Ebar_{ij}
&=& 2 \Bigg[
\left\langle \left( \psiijbypsi + {\psii\psij\over \psisq} \right) (\EL-\Ebar) \right\rangle \nonumber \\
&&
-\left\langle \psiibypsi \right\rangle \Ej
-\left\langle \psijbypsi \right\rangle \Ei
+ \left\langle \psiibypsi \ELj \right\rangle \Bigg].
\label{rappe}
\eeq
Note that many conjugate gradient approaches do need the Hessian (the functions to be discussed next week are examples of this)!
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Simple example and demonstration}
\begin{small}
{\scriptsize
We can also minimize the variance. In our simple model the variance is
\[
\sigma^2[\alpha] = \frac{1}{2}\alpha^4-\frac{1}{4}+\frac{1}{32\alpha^4},
\]
with first derivative 
\[
\frac{d \sigma^2[\alpha]}{d\alpha} = 2\alpha^3-\frac{1}{8\alpha^5}
\]
and a second derivative which is always positive 
\[
\frac{d^2\sigma^2[\alpha]}{d\alpha^2} = 6\alpha^2+\frac{5}{8\alpha^6}
\]
Some professional codes include both a variance and an energy variation.
}

\end{small}
}




\frame
{
  \frametitle{Conjugate gradient method, our case}
\begin{small}
{\scriptsize
In Newton's method we set $\nabla f = 0$ and we can thus compute the next iteration point
(here the exact result)
\[
\hat{\bf {x}}-\hat{\bf {x}}_i=\hat{\bf {A}}^{-1}\nabla f(\hat{\bf {x}}_i).
\]
Subtracting this equation from that of $\hat{\bf {x}}_{i+1}$ we have
\[
\hat{\bf {x}}_{i+1}-\hat{\bf {x}}_i=\hat{\bf {A}}^{-1}(\nabla f(\hat{\bf {x}}_{i+1})-\nabla f(\hat{\bf {x}}_i)).
\]
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple example and demonstration}
\begin{small}
{\scriptsize
In our case $f$ can be either the energy or the variance.  If we choose the energy then we have 
\[
\hat{\bf {\alpha}}_{i+1}-\hat{\bf {\alpha}}_i=\hat{\bf {A}}^{-1}(\nabla E(\hat{\bf {\alpha}}_{i+1})-\nabla E(\hat{\bf {\alpha}}_i)).
\]
In the simple model gradient and the Hessian $\hat{\bf A}}$ are 
\[
\frac{d\langle  E_L[\alpha]\rangle}{d\alpha} = \alpha-\frac{1}{4\alpha^3}
\]
and a second derivative which is always positive (meaning that we find a minimum)
\[
\hat{\bf A}}= \frac{d^2\langle  E_L[\alpha]\rangle}{d\alpha^2} = 1+\frac{3}{4\alpha^4}
\]
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Simple example and demonstration}
\begin{small}
{\scriptsize
We get then
\[
\alpha_{i+1}=\frac{4}{3}\alpha_i-\frac{\alpha_i^4}{3\alpha_{i+1}^3},
\]
which can be rewritten as
\[
\alpha_{i+1}^4-\frac{4}{3}\alpha_i\alpha_{i+1}^4+\frac{1}{3}\alpha_i^4.
\]
}
\end{small}
}




\frame
{
  \frametitle{Using the conjugate gradient method}
\begin{small}
{\scriptsize
\begin{itemize}
\item Start your program with calling a function which implements for example the  CGM method %(function $dfpmin$).
\item This function needs the function for the expectation value of the local energy and
the derivative of the local energy.  
\item Your function $func$ is now the Metropolis part with a call to the local energy function.
For every call to the function $func$ many practitionser use 1000-10000 Monte Carlo cycles for the trial wave function.
\item This gives an expectation value for the energy which is returned by the function $func$.
\item When one calls the local energy one also computes the first derivative of the expectaction value of the local energy
\[
\frac{d\langle E_{L}[\alpha] \rangle}{d\alpha}=
2\left\langle \psiibypsi (E_{L}[\alpha] - \langle E_{L}[\alpha] \rangle) \right\rangle.
\]
\end{itemize}
}
\end{small}
}



\frame
{
  \frametitle{Using the conjugate gradient method}
\begin{small}
{\scriptsize
The expectation value for the local energy of the Helium atom with a simple Slater determinant is given by
\[
\langle E_{L} \rangle = \alpha^2-2\alpha\left(Z-\frac{5}{16}\right)
\]
When implementing the conjugate gradient method, uyou should test your numerical derivative with the derivative of the last expression, that is
\[
\frac{d\langle E_{L}[\alpha] \rangle}{d\alpha} = 2\alpha-2\left(Z-\frac{5}{16}\right).
\]
}
\end{small}
}

\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
In the CG method we define so-called conjugate directions and two vectors 
$\hat{\bf {s}}$ and $\hat{\bf {t}}$
are said to be
conjugate if 
\[
\hat{\bf {s}}^T\hat{{\bf A}}\hat{\bf {t}}= 0.
\]
The philosophy of the CG method is to perform searches in various conjugate directions
of our vectors $\hat{{\bf x}}_i$ obeying the above criterion, namely
\[
\hat{\bf {x}}_i^T\hat{{\bf A}}\hat{\bf {x}}_j= 0.
\]
Two vectors are conjugate if they are orthogonal with respect to 
this inner product. Being conjugate is a symmetric relation: if $\hat{\bf {s}}$ is conjugate to $\hat{\bf {t}}$, then $\hat{\bf {t}}$ is conjugate to $\hat{\bf {s}}$.
}
\end{small}
}


\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
An example is given by the eigenvectors of the matrix 
\[
\hat{\bf {v}}_i^T\hat{{\bf A}}\hat{\bf {v}}_j= \lambda\hat{\bf {v}}_i^T\hat{\bf {v}}_j,
\]
which is zero unless $i=j$. 

}
\end{small}
}


\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
Assume now that we have a symmetric positive-definite matrix $\hat{\bf {A}}$ of size
$n\times n$. At each iteration $i+1$ we obtain the conjugate direction of a vector 
\[
\hat{\bf {x}}_{i+1}=\hat{\bf {x}}_{i}+\alpha_i\hat{\bf {p}}_{i}. 
\]
We assume that $\hat{\bf {p}}_{i}$ is a sequence of $n$ mutually conjugate directions. 
Then the $\hat{\bf {p}}_{i}$  form a basis of $R^n$ and we can expand the solution 
$  \hat{{\bf A}}\hat{\bf {x}} = \hat{\bf {b}}$ in this basis, namely
\[
  \hat{\bf {x}}  = \sum^{n}_{i=1} \alpha_i \hat{\bf {p}}_i.
\]
}
\end{small}
}

\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
The coefficients are given by
\[
    \mathbf{A}\mathbf{x} = \sum^{n}_{i=1} \alpha_i \mathbf{A} \mathbf{p}_i = \mathbf{b}.
\]
Multiplying with $\hat{\bf {p}}_k^T$  from the left gives
\[
  \hat{\bf {p}}_k^T \hat{\bf {A}}\hat{\bf {x}} = \sum^{n}_{i=1} \alpha_i\hat{\bf {p}}_k^T \hat{\bf {A}}\hat{\bf {p}}_i= \hat{\bf {p}}_k^T \hat{\bf {b}},
\]
and we can define the coefficients $\alpha_k$ as 
\[
    \alpha_k = \frac{\hat{\bf {p}}_k^T \hat{\bf {b}}}{\hat{\bf {p}}_k^T \hat{\bf {A}} \hat{\bf {p}}_k}
\] 
}
\end{small}
}

\frame
{
  \frametitle{Conjugate gradient method and iterations}
\begin{small}
{\scriptsize
If we choose the conjugate vectors $\hat{\bf {p}}_k$ carefully, 
then we may not need all of them to obtain a good approximation to the solution 
$\hat{\bf {x}}$. 
So, we want to regard the conjugate gradient method as an iterative method. 
This also allows us to solve systems where $n$ is so large that the direct 
method would take too much time.

We denote the initial guess for $\hat{\bf {x}}$ as $\hat{\bf {x}}_0$. 
We can assume without loss of generality that 
\[
\hat{\bf {x}}_0=0,
\]
or consider the system 
\[
\hat{\bf {A}}\hat{\bf {z}} = \hat{\bf {b}}-\hat{\bf {A}}\hat{\bf {x}}_0,
\]
instead.
}
\end{small}
}


\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
Important, one can show that the solution $\hat{\bf {x}}$ is also the unique minimizer of the quadratic form
\[
  f(\hat{\bf {x}}) = \frac{1}{2}\hat{\bf {x}}^T\hat{\bf {A}}\hat{\bf {x}} - \hat{\bf {x}}^T \hat{\bf {x}} , \quad \hat{\bf {x}}\in\mathbf{R}^n. 
\]
This suggests taking the first basis vector $\hat{\bf {p}}_1$ 
to be the gradient of $f$ at $\hat{\bf {x}}=\hat{\bf {x}}_0$, 
which equals 
\[
\hat{\bf {A}}\hat{\bf {x}}_0-\hat{\bf {b}},
\]
and 
$\hat{\bf {x}}_0=0$ it is equal $-\hat{\bf {b}}$.
The other vectors in the basis will be conjugate to the gradient, 
hence the name conjugate gradient method.
}
\end{small}
}


\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
Let  $\hat{\bf {r}}_k$ be the residual at the $k$-th step:
\[
\hat{\bf {r}}_k=\hat{\bf {b}}-\hat{\bf {A}}\hat{\bf {x}}_k.
\]

Note that $\hat{\bf {r}}_k$ is the negative gradient of $f$ at 
$\hat{\bf {x}}=\hat{\bf {x}}_k$, 
so the gradient descent method would be to move in the direction $\hat{\bf {r}}_k$. 
Here, we insist that the directions $\hat{\bf {p}}_k$ are conjugate to each other, 
so we take the direction closest to the gradient $\hat{\bf {r}}_k$  
under the conjugacy constraint. 
This gives the following expression
\[
\hat{\bf {p}}_{k+1}=\hat{\bf {r}}_k-\frac{\hat{\bf {p}}_k^T \hat{\bf {A}}\hat{\bf {r}}_k}{\hat{\bf {p}}_k^T\hat{\bf {A}}\hat{\bf {p}}_k} \hat{\bf {p}}_k.
\]
}
\end{small}
}

\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
We can also  compute the residual iteratively as
\[
\hat{\bf {r}}_{k+1}=\hat{\bf {b}}-\hat{\bf {A}}\hat{\bf {x}}_{k+1},
 \]
which equals
\[
\hat{\bf {b}}-\hat{\bf {A}}(\hat{\bf {x}}_k+\alpha_k\hat{\bf {p}}_k),
 \]
or
\[
(\hat{\bf {b}}-\hat{\bf {A}}\hat{\bf {x}}_k)-\alpha_k\hat{\bf {A}}\hat{\bf {p}}_k,
 \]
which gives
\[
\hat{\bf {r}}_{k+1}=\hat{\bf {r}}_k-\hat{\bf {A}}\hat{\bf {p}}_{k},
 \]
}
\end{small}
}



\frame
{
  \frametitle{Conjugate gradient method, our case}
\begin{small}
{\scriptsize
If we consider finding the minimum of a function $f$ using Newton's method,
that is search for a zero of the gradient of a function.  Near a point $x_i$
we have to second order
\[
f(\hat{\bf {x}})=f(\hat{\bf {x}}_i)+(\hat{\bf {x}}-\hat{\bf {x}}_i)\nabla f(\hat{\bf {x}}_i)
\frac{1}{2}(\hat{\bf {x}}-\hat{\bf {x}}_i)\hat{\bf {A}}(\hat{\bf {x}}-\hat{\bf {x}}_i)
\]
giving
\[
\nabla f(\hat{\bf {x}})=\nabla f(\hat{\bf {x}}_i)+\hat{\bf {A}}(\hat{\bf {x}}-\hat{\bf {x}}_i).
 \]
In Newton's method we set $\nabla f = 0$ and we can thus compute the next iteration point
(here the exact result)
\[
\hat{\bf {x}}-\hat{\bf {x}}_i=\hat{\bf {A}}^{-1}\nabla f(\hat{\bf {x}}_i).
\]
Subtracting this equation from that of $\hat{\bf {x}}_{i+1}$ we have
\[
\hat{\bf {x}}_{i+1}-\hat{\bf {x}}_i=\hat{\bf {A}}^{-1}(\nabla f(\hat{\bf {x}}_{i+1})-\nabla f(\hat{\bf {x}}_i)).
\]
}
\end{small}
}


\frame
{
  \frametitle{The simple model}
%\vspace{-3cm}
      \begin{figure}[htp]
        \includegraphics[width=0.8\textwidth]{pxy.png}
      \end{figure}
}


\frame
{
  \frametitle{The simple model, contour plots}
%\vspace{-3cm}
      \begin{figure}[htp]
        \includegraphics[width=0.8\textwidth]{contours.png}
      \end{figure}
}


\frame
{
  \frametitle{The simple model, the derivatives}
%\vspace{-3cm}
      \begin{figure}[htp]
        \includegraphics[width=0.8\textwidth]{derivatives.png}
      \end{figure}
}


\frame
{
  \frametitle{The simple model, steepest descent steps}
%\vspace{-3cm}
      \begin{figure}[htp]
        \includegraphics[width=0.8\textwidth]{directions.png}
      \end{figure}
}



\frame[containsverbatim]
{
  \frametitle{Simple codes for  steepest descent and conjugate gradient }
\begin{small}
{\scriptsize
\begin{verbatim}
#include <cmath>
#include <iostream>
#include <fstream>
#include <iomanip>
#include "vectormatrixclass.h"
using namespace  std;
//   Main function begins here
int main(int  argc, char * argv[]){
  int dim = 2;
  Vector x(dim),xsd(dim), b(dim),x0(dim);
  Matrix A(dim,dim);
  
  // Set our initial guess
  x0(0) = x0(1) = 0;
  // Set the matrix  
  A(0,0) =  3;    A(1,0) =  2;   A(0,1) =  2;   A(1,1) =  6; 
  b(0) = 2; b(1) = -8;
\end{verbatim}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple codes for  steepest descent and conjugate gradient }
\begin{small}
{\scriptsize
\begin{verbatim}
  cout << "The Matrix A that we are using: " << endl;
  A.Print();
  cout << endl;


  x = ConjugateGradient(A,b,x0);

  xsd = SteepestDescent(A,b,x0);
  
  cout << "The approximate solution using Conjugate Gradient is: " << endl;
  x.Print();
  cout << endl;

  cout << "The approximate solution using Steepest Descent is: " << endl;
  xsd.Print();
  cout << endl;
}
\end{verbatim}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple codes for  steepest descent and conjugate gradient }
\begin{small}
{\scriptsize
\begin{verbatim}
Vector SteepestDescent(Matrix A, Vector b, Vector x0){
  int IterMax, i;
  int dim = x0.Dimension();
  const double tolerance = 1.0e-14;
  Vector x(dim),f(dim),z(dim);
  double c,alpha,d;
  IterMax = 30;
  x = x0;
  f = A*x-b;
  i = 0;
\end{verbatim}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Simple codes for  steepest descent and conjugate gradient }
\begin{small}
{\scriptsize
\begin{verbatim}
  while (i <= IterMax){
    z = A*f;
    c = dot(f,f);
    alpha = c/dot(f,z);
    x = x - alpha*f;
    f =  A*x-b;
    if(sqrt(dot(f,f)) < tolerance) break;
    i++;
  }
  return x;
} 
\end{verbatim}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Simple codes for  steepest descent and conjugate gradient }
\begin{small}
{\scriptsize
\begin{verbatim}
  Vector ConjugateGradient(Matrix A, Vector b, Vector x0){
  int dim = x0.Dimension();
  const double tolerance = 1.0e-14;
  Vector x(dim),r(dim),v(dim),z(dim);
  double c,t,d;

  x = x0;
  r = b - A*x;
  v = r;
  c = dot(r,r);
\end{verbatim}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Simple codes for  steepest descent and conjugate gradient }
\begin{small}
{\scriptsize
\begin{verbatim}
  int i = 0; IterMax = dim;
  while(i <= IterMax){
    z = A*v;
    t = c/dot(v,z);
    x = x + t*v;
    r = r - t*z;
    d = dot(r,r);
    if(sqrt(d) < tolerance)
      break;
    v = r + (d/c)*v;
    c = d;  i++;
  }
  return x;
} 
\end{verbatim}
}
\end{small}
}






\frame[containsverbatim]
{
  \frametitle{Codes from numerical recipes}
\begin{small}
{\scriptsize
The codes are taken from chapter 10.7 of Numerical recipes.  We use the functions
$dfpmin$ and $lnsrch$.  You can load down the package of programs from the webpage of
the course, see under project 1.  
The package is called $NRcgm107.tar.gz$ and contains the files 
$dfmin.c$, $lnsrch.c$, $nrutil.c$ and $nrutil.h$. 
These codes are  written in C. 
\begin{verbatim}

void dfpmin(double p[], int n, double gtol, int *iter, double *fret,
double(*func)(double []), void (*dfunc)(double [], double []))

\end{verbatim}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{What you have to provide}
\begin{small}
{\scriptsize
The input to $dfpmin$
\begin{verbatim}

void dfpmin(double p[], int n, double gtol, int *iter, double *fret,
double(*func)(double []), void (*dfunc)(double [], double []))

\end{verbatim}
is
\begin{itemize}
\item The starting vector $p$ of length $n$
\item The function $func$ on which minimization is done
\item The function $dfunc$ where the gradient i calculated
\item The convergence requirement for zeroing the gradient $gtol$.
\end{itemize}
It returns in $p$ the location of the minimum, the number of iterations and 
the minimum value of the function under study $fret$.
}
\end{small}
}










\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
#include "nrutil.h"
using namespace std;
//     Here we define various functions called by the main program

double E_function(double *x);
void   dE_function(double *x, double *g);
void   dfpmin(double p[], int n, double gtol, int *iter, double *fret,
	    double(*func)(double []), void (*dfunc)(double [], double []));
//   Main function begins here
int main()
{
     int n, iter;
     double gtol, fret;
     double alpha;
     n = 1;
     cout << "Read in guess for alpha" << endl;
     cin >> alpha;
\end{verbatim}
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
//   reserve space in memory for vectors containing the variational
//   parameters
     double *p = new double [2];
     gtol = 1.0e-5;
//   now call dfmin and compute the minimum
     p[1] = alpha;
     dfpmin(p, n, gtol, &iter, &fret,&E_function,&dE_function);
     cout << "Value of energy minimum = " << fret << endl;
     cout << "Number of iterations = " << iter << endl;
     cout << "Value of alpha at minimu = " << p[1] << endl;
      delete [] p;
\end{verbatim}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
//  this function defines the Energy function
double E_function(double x[])
{
  double value = x[1]*x[1]*0.5+1.0/(8*x[1]*x[1]);
  return value;
} // end of function to evaluate
\end{verbatim}
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
//  this function defines the derivative of the energy 
void dE_function(double x[], double g[])
{
  g[1] = x[1]-1.0/(4*x[1]*x[1]*x[1]);
} // end of function to evaluate
\end{verbatim}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
using namespace std;
//     Here we define various functions called by the main program

double E_function(double *x);
void   dE_function(double *x, double *g);
void   dfpmin(double p[], int n, double gtol, int *iter, double *fret,
	    double(*func)(double []), void (*dfunc)(double [], double []));
//   Main function begins here
int main()
{
     int n, iter;
     double gtol, fret;
     double alpha;
     n = 1;
     cout << "Read in guess for alpha" << endl;
     cin >> alpha;
\end{verbatim}
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
//   reserve space in memory for vectors containing the variational
//   parameters
     double *p = new double [2];
     gtol = 1.0e-5;
//   now call dfmin and compute the minimum
     p[1] = alpha;
     dfpmin(p, n, gtol, &iter, &fret,&E_function,&dE_function);
     cout << "Value of energy minimum = " << fret << endl;
     cout << "Number of iterations = " << iter << endl;
     cout << "Value of alpha at minimu = " << p[1] << endl;
      delete [] p;
\end{verbatim}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
//  this function defines the Energy function
double E_function(double x[])
{

//  Change here by calling your Metropolis function which 
//  returns the local energy

  double value = x[1]*x[1]*0.5+1.0/(8*x[1]*x[1]);



  return value;
} // end of function to evaluate
\end{verbatim}
You need to change this function so that you call the local energy for your system. I used 1000
cycles per call to get a new value of $\langle E_L[\alpha]\rangle$.

}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
//  this function defines the derivative of the energy 
void dE_function(double x[], double g[])
{

//  Change here by calling your Metropolis function. 
//  I compute both the local energy and its derivative for every call to func

  g[1] = x[1]-1.0/(4*x[1]*x[1]*x[1]);
} // end of function to evaluate
\end{verbatim}
You need to change this function so that you call the local energy for your system. I used 1000
cycles per call to get a new value of $\langle E_L[\alpha]\rangle$.
When I compute the local energy I also compute its derivative.
After roughly 10-20 iterations I got a converged result in terms of $\alpha$.
}
\end{small}
}


\frame
{
  \frametitle{The hydrogen molecule, our next step}
\begin{small}
{\scriptsize
The 
H$_2$ molecule consists of two protons and two electrons 
with a ground state energy $E=-31.949$ eV or $-1.175$ a.u. and the 
equilibrium distance between the two hydrogen atoms
of $r_0=1.40$ Bohr radii (recall that a Bohr radius is $0.05\times 10^{-9}$m.


We define our systems using the following variables.
Origo is chosen to be halfway between the two protons. The distance from 
proton 1 is defined as 
$-{\bf R}/2$ whereas proton 2 has a distance ${\bf R}/2$.
Calculations are performed for fixed distances ${\bf R}$ between the two protons.

}
\end{small}
}


\frame
{
  \frametitle{The hydrogen molecule, our next step}
\begin{small}
{\scriptsize
Electron 1 has a distance $r_1$ from the chose origo, while  electron $2$
has a distance $r_2$. 
The kinetic energy operator becomes then
\[
   -\frac{\nabla_1^2}{2}-\frac{\nabla_2^2}{2}.
\]
The distance between the two electrons is
$r_{12}=|{\bf r}_1-{\bf r}_2|$. 
The repulsion between the two electrons results in a potential energy term given by
\[
               +\frac{1}{r_{12}}.
\]
In a similar way we obtain a repulsive contribution from the interaction between the two 
protons given by
\[
               +\frac{1}{|{\bf R}|},
\]
where ${\bf R}$ is the distance between the two protons.
}
\end{small}
}


\frame
{
  \frametitle{The hydrogen molecule, our next step}
\begin{small}
{\scriptsize
To obtain the final potential energy we need to include the attraction the electrons feel from the protons.
To model this, we need to define the distance between the electrons and the two protons.
If we model this along a 
chosen $z$-akse with electron 1 placed at a distance 
${\bf r}_1$ from a chose origo, one proton at $-{\bf R}/2$
and the other at  ${\bf R}/2$, 
the distance from proton 1 to electron 1 becomes
\[
{\bf r}_{1p1}={\bf r}_1+ {\bf R}/2,
\]
and
\[
{\bf r}_{1p2}={\bf r}_1- {\bf R}/2,
\]
from proton 2.
}
\end{small}
}


\frame
{
  \frametitle{The hydrogen molecule, our next step}
\begin{small}
{\scriptsize
Similarly, for electron 2 we obtain
\[
{\bf r}_{2p1}={\bf r}_2+{\bf R}/2,
\]
and
\[
{\bf r}_{2p2}={\bf r}_2-{\bf R}/2.
\]
These four distances define the attractive contributions to the potential energy
\[
   -\frac{1}{r_{1p1}}-\frac{1}{r_{1p2}}-\frac{1}{r_{2p1}}-\frac{1}{r_{2p2}}.
\]
We can then write the total Hamiltonian as 
\[
   \OP{H}=-\frac{\nabla_1^2}{2}-\frac{\nabla_2^2}{2}
   -\frac{1}{r_{1p1}}-\frac{1}{r_{1p2}}-\frac{1}{r_{2p1}}-\frac{1}{r_{2p2}}
               +\frac{1}{r_{12}}+\frac{1}{|{\bf R}|}.
\]
}
\end{small}
}
\frame
{
  \frametitle{The hydrogen molecule, our next step}
\begin{small}
{\scriptsize
We will use a trial wave function of the form
\[
   \psi_{T}({\bf r_1},{\bf r_2}, {\bf R}) =
   \psi({\bf r}_1,{\bf R})\psi({\bf r}_2,{\bf R})
   \exp{\left(\frac{r_{12}}{2(1+\beta r_{12})}\right)},
\]
with the following trial wave function 
\[
   \psi({\bf r}_1,{\bf R})=\left(\exp{(-\alpha r_{1p1})}
      +\exp{(-\alpha r_{1p2})}\right),
\]
for electron 1 and
\[
   \psi({\bf r}_2,{\bf R})=\left(\exp{(-\alpha r_{2p1})}
      +\exp{(-\alpha r_{2p2})}\right).
\]
The variational parameters are $\alpha$ and $\beta$.

}
\end{small}
}
\frame
{
  \frametitle{The Be$_2$ molecule, our final step}
\begin{small}
{\scriptsize
Our final step consists in estimating the binding energy of the Be$_2$ molecule.
Useful references are then
\begin{enumerate}
\item Moskowitz and Kalos, Int.~Journal of Quantum Chemistry {\bf XX}, 1107 (1981).
Results for He and H$_2$.
\item Filippi, Singh and Umrigar, J.~Chemical Physics {\bf 105}, 123 (1996).   Useful results on
Be$_2$ to which you can benchmark your results against.
\end{enumerate}
}
\end{small}
}

\end{document}







